\chapter{Neuro-Symbolic Verification of Safety Critical Systems}
\label{chap:ansya}

Formal verification is a collection of symbolic techniques aimed at proving the correctness of a system with respect to some specification. Although extremely important for domains such as Safety-critical Systems, verification is an expensive process, often intractable for larger instances. Moreover, the fully-symbolic nature, makes (sub-symbolic) perception a blind spot of formal verification. In this chapter we employ the \textsc{LTLZinc-Safety} datasets to explore a simple Neuro-symbolic approach to efficiently approximating the verification process by means of Sequence Classification, while preserving the ability to adapt to perceptual stimuli.
%
The content of this chapter is a significant extension of our ANSyA2025 workshop paper~\cite{lorello2025ansya}.

\section{Background}
Safety-critical Systems are domains in which failure might produce significant damage to the system itself or to the environment, or even loss of life~\cite{knight2002safety}. Typical application areas include healthcare, for critical tasks such as the monitoring of biomedical devices~\cite{vakhter2022threat}, transportation, as in the case of aircraft flight control systems~\cite{stolzer2023safety}, or space missions, for the detection of anomalies and cyber-security vulnerabilities~\cite{vessels2019cybersecurity}. In most of these scenarios, safety must be ensured by assessing that the behavior of the system is compliant with strict constraints, such as time constraints (e.g., tasks that must complete their executions within certain time limits) or logic constraints (e.g., events that must occur in a predefined sequential order)~\cite{graydon2014realistic,favaro2018application}. Many formalisms can be used to model these systems, such as Stochastic Time Petri nets~\cite{carnevali2012quantitative}, Deterministic or Symbolic Finite-state Automata~\cite{giantamidis2020efficient}, Linear Temporal Logic (\LTL)~\cite{ma2010approach}, Fault Trees~\cite{roth2013modeling,ruijters2015fault} and others, with the aim of quantitatively evaluating  dependability attributes~\cite{maurya2020reliability,carnevali2025faultflow}.
In this chapter we explore the possibility of casting the problem of verification of Safety-critical Systems into an \textsc{LTLZinc} problem. 
%
Consider, for instance, a Safety-critical System composed of two devices ($A$ and $B$) characterized by ten possible states $\gY \in [0, 9]$, each associated with a perceptual signature, in the form of audio spectrograms $\mathcal{X} \in \left\{\mathimg{urban_0.png}, \ldots, \mathimg{urban_9.png}\right\}$. Suppose this system must comply to a \textit{liveness} property~\cite{alpern1985defining} $\gF$, asserting that an event $p$ (``the sensor is in state 4'') registered by sensor $A$ must always be followed by another event $q$ (``the sensor is in state 7''), observed by sensor $B$. Events can be tracked, by systematically evaluating the validity of a set of relational predicates $\mathcal{C}$, on the state of the system over time.
The behavior described above can be represented compactly by the following \textsc{LTLZinc} specification:
\begin{align*}
	\begin{split}
		\mathcal{X}\colon & A, B = \left\{\mathimg{urban_0.png}, \mathimg{urban_1.png}, \mathimg{urban_2.png}, \mathimg{urban_3.png}, \mathimg{urban_4.png}, \mathimg{urban_5.png}, \mathimg{urban_6.png}, \mathimg{urban_7.png}, \mathimg{urban_8.png}, \mathimg{urban_9.png}\right\}\\
		\mathcal{Y}\colon &A, B = [0, 9]\\
		\mathcal{C}\colon &\texttt{p}(Z): Z = 4, \\
		& \texttt{q}(Z): Z = 7\\
		\mathcal{F}\colon & \ltlglobally (\texttt{p}(A) \rightarrow \ltlfinally \texttt{q}(B)).
	\end{split}
\end{align*}
In this setting, the verification of traces of events generated by the Safety Critical System, can be cast into a Sequence Classification with background knowledge setting, and addressed by the same Neuro-symbolic pipeline introduced in Chapter~\ref{chap:ltlzincseq}.
%
\textsc{LTLZinc} enables the design of experiments in different learning and reasoning scenarios, depending on what kind of knowledge is available at training time, and which element of the tuple  $\langle \gX, \gY, \gC, \gF \rangle$ constitutes the learning objective. For example, in many cases, a Safety-critical System is designed with predefined specifications in mind, and it is therefore reasonable to assume prior knowledge about both $\gC$ and $\gF$, but not the mapping $\gX \mapsto \gY$.
%Within this setting, a \textit{sequence classification} task corresponds to predict whether a given sequence of perceptual stimuli $\gX^{(t)}$ along a discrete set of $n$ time-steps $t = \{1, \ldots, n\}$ satisfies $\gF$ or not,  and it corresponds to the \textit{verification} of $\mathcal{F}$ within a given sequence $\mathcal{X}^t$, by neuro-symbolic means. 
In other cases, either $\mathcal{C}$ or $\mathcal{F}$ could be unknown, and the sequence classification task would thus involve the \textit{induction} of temporal safety properties directly from system traces: in these settings, the Neuro-symbolic system is trained to discriminate between positive and negative sequences, without knowing neither $\gF$ nor $\gC$.

\section{Experiments}
We perform experiments on \textsc{LTLZinc-Safety-Prop} and \textsc{LTLZinc-Safety-FOL} by employing the modular pipeline described in Chapter~\ref{chap:ltlzincseq}.
As the perceptual component of \textsc{LTLZinc-Safety} is significantly more challenging than the one of \textsc{LTLZinc-Sequential}, we replace the \textsc{ic} module with a ResNet18~\cite{he2016deep}, pre-trained on ImageNet. 
Images are augmented during training and inference, according to the original ResNet18 transforms.
The \textsc{cc} and \textsc{nsp} modules are jointly initialized in two flavors: (i.) \textit{Symbolic} (Problog \textsc{cc}, Fuzzy \textsc{nsp}), and (ii.) \textit{Neural} (MLP-L \textsc{cc}, GRU-L \textsc{nsp}).
%
After selecting optimal hyper-parameters (optimizer: Adam, learning rate: $10^{-4}$) on a simplified task (the one introduced at the beginning of the chapter), we initialize the \textsc{IC} module with one epoch of pre-training on image labels only, then provide supervisions at every stage ($\lambda_{\textsc{CC}} = \lambda_{\textsc{NSP}} = \lambda_{\textsc{SC}} = 1.0, \lambda_\textsc{IC} = 0.1$). For either dataset, we perform two batches of experiments, to assess the effect of training budget: (i.) \textit{low training budget} (1 epoch of training), and (ii.) \textit{high training budget} (7 epochs of training).

\paragraph{Results on \textsc{LTLZinc-Safety-Prop}.}
\begin{table*}[!t]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{cccccccc}
			\toprule
			\multirow{2}{*}{\sc Task} & \multirow{2}{*}{\sc Category} & \multirow{2}{*}{\shortstack[c]{\sc Best Epoch}} & \multirow{2}{*}{\shortstack[c]{\sc Average\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Label\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Constraint\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Successor\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Sequence\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}}\\
			& & & & & & & \\
			\midrule
			\multirow{2}{*}{Task 1} & Symbolic & $0$ & $\textbf{0.67} $ {\tiny ($\pm 0.03$)} & $\textbf{0.72} $ {\tiny ($\pm 0.02$)} & $\textbf{0.82} $ {\tiny ($\pm 0.02$)} & $\textbf{0.58} $ {\tiny ($\pm 0.05$)} & $\textbf{0.55} $ {\tiny ($\pm 0.06$)}\\
			& Neural & $1$ & $0.61 $ {\tiny ($\pm 0.00$)} & $0.69 $ {\tiny ($\pm 0.02$)} & $0.78 $ {\tiny ($\pm 0.02$)} & $0.44 $ {\tiny ($\pm 0.00$)} & $0.52 $ {\tiny ($\pm 0.05$)}\\
			\hdashline
			\multirow{2}{*}{Task 2} & Symbolic & $0$ & $\textbf{0.65} $ {\tiny ($\pm 0.03$)} & $\textbf{0.70} $ {\tiny ($\pm 0.05$)} & $\textbf{0.81} $ {\tiny ($\pm 0.05$)} & $0.57 $ {\tiny ($\pm 0.03$)} & $0.51 $ {\tiny ($\pm 0.01$)}\\
			& Neural & $1$ & $\textbf{0.65} $ {\tiny ($\pm 0.05$)} & $0.62 $ {\tiny ($\pm 0.06$)} & $0.77 $ {\tiny ($\pm 0.04$)} & $\textbf{0.61} $ {\tiny ($\pm 0.10$)} & $\textbf{0.60} $ {\tiny ($\pm 0.09$)}\\
			\hdashline
			\multirow{2}{*}{Task 3} & Symbolic & $0$ & $\textbf{0.62} $ {\tiny ($\pm 0.02$)} & $\textbf{0.62} $ {\tiny ($\pm 0.04$)} & $\textbf{0.77} $ {\tiny ($\pm 0.02$)} & $\textbf{0.57} $ {\tiny ($\pm 0.02$)} & $\textbf{0.51} $ {\tiny ($\pm 0.01$)}\\
			& Neural & $1$ & $0.54 $ {\tiny ($\pm 0.05$)} & $0.61 $ {\tiny ($\pm 0.08$)} & $\textbf{0.77} $ {\tiny ($\pm 0.07$)} & $0.31 $ {\tiny ($\pm 0.08$)} & $0.48 $ {\tiny ($\pm 0.06$)}\\
			\hdashline
			\multirow{2}{*}{Task 4} & Symbolic & $0$ & $\textbf{0.61} $ {\tiny ($\pm 0.02$)} & $\textbf{0.67} $ {\tiny ($\pm 0.04$)} & $\textbf{0.79} $ {\tiny ($\pm 0.03$)} & $0.47 $ {\tiny ($\pm 0.02$)} & $0.49 $ {\tiny ($\pm 0.01$)}\\
			& Neural & $1$ & $\textbf{0.61} $ {\tiny ($\pm 0.02$)} & $0.63 $ {\tiny ($\pm 0.03$)} & $0.78 $ {\tiny ($\pm 0.01$)} & $\textbf{0.53} $ {\tiny ($\pm 0.04$)} & $\textbf{0.52} $ {\tiny ($\pm 0.06$)}\\
			\hdashline
			\multirow{2}{*}{Task 5} & Symbolic & $0$ & $\textbf{0.74} $ {\tiny ($\pm 0.03$)} & $\textbf{0.69} $ {\tiny ($\pm 0.04$)} & $\textbf{0.79} $ {\tiny ($\pm 0.02$)} & $\textbf{0.74} $ {\tiny ($\pm 0.01$)} & $\textbf{0.75} $ {\tiny ($\pm 0.04$)}\\
			& Neural & $1$ & $0.71 $ {\tiny ($\pm 0.05$)} & $0.60 $ {\tiny ($\pm 0.06$)} & $0.77 $ {\tiny ($\pm 0.04$)} & $0.73 $ {\tiny ($\pm 0.05$)} & $0.74 $ {\tiny ($\pm 0.07$)}\\
			\hdashline
			\multirow{2}{*}{Task 6} & Symbolic & $0$ & $\textbf{0.69} $ {\tiny ($\pm 0.06$)} & $\textbf{0.68} $ {\tiny ($\pm 0.04$)} & $0.80 $ {\tiny ($\pm 0.03$)} & $\textbf{0.64} $ {\tiny ($\pm 0.08$)} & $\textbf{0.67} $ {\tiny ($\pm 0.10$)}\\
			& Neural & $1$ & $0.62 $ {\tiny ($\pm 0.02$)} & $0.67 $ {\tiny ($\pm 0.05$)} & $\textbf{0.82} $ {\tiny ($\pm 0.02$)} & $0.49 $ {\tiny ($\pm 0.02$)} & $0.50 $ {\tiny ($\pm 0.03$)}\\
			\hdashline
			\multirow{2}{*}{Task 7} & Symbolic & $0$ & $\textbf{0.75} $ {\tiny ($\pm 0.02$)} & $\textbf{0.68} $ {\tiny ($\pm 0.04$)} & $\textbf{0.82} $ {\tiny ($\pm 0.01$)} & $\textbf{0.75} $ {\tiny ($\pm 0.02$)} & $\textbf{0.75} $ {\tiny ($\pm 0.02$)}\\
			& Neural & $1$ & $0.69 $ {\tiny ($\pm 0.03$)} & $0.59 $ {\tiny ($\pm 0.03$)} & $0.78 $ {\tiny ($\pm 0.03$)} & $0.72 $ {\tiny ($\pm 0.02$)} & $0.70 $ {\tiny ($\pm 0.06$)}\\
			\hdashline
			\multirow{2}{*}{Task 8} & Symbolic & $0$ & $0.61 $ {\tiny ($\pm 0.02$)} & $\textbf{0.64} $ {\tiny ($\pm 0.03$)} & $0.78 $ {\tiny ($\pm 0.05$)} & $0.47 $ {\tiny ($\pm 0.02$)} & $0.54 $ {\tiny ($\pm 0.01$)}\\
			& Neural & $1$ & $\textbf{0.67} $ {\tiny ($\pm 0.01$)} & $\textbf{0.64} $ {\tiny ($\pm 0.02$)} & $\textbf{0.79} $ {\tiny ($\pm 0.02$)} & $\textbf{0.54} $ {\tiny ($\pm 0.02$)} & $\textbf{0.70} $ {\tiny ($\pm 0.02$)}\\
			\hdashline
			\multirow{2}{*}{Task 9} & Symbolic & $0$ & $0.63 $ {\tiny ($\pm 0.04$)} & $\textbf{0.68} $ {\tiny ($\pm 0.03$)} & $\textbf{0.82} $ {\tiny ($\pm 0.01$)} & $0.48 $ {\tiny ($\pm 0.07$)} & $0.56 $ {\tiny ($\pm 0.04$)}\\
			& Neural & $1$ & $\textbf{0.67} $ {\tiny ($\pm 0.01$)} & $0.59 $ {\tiny ($\pm 0.01$)} & $0.79 $ {\tiny ($\pm 0.02$)} & $\textbf{0.57} $ {\tiny ($\pm 0.01$)} & $\textbf{0.71} $ {\tiny ($\pm 0.01$)}\\
			\bottomrule
		\end{tabular}
	}
	\caption[Low training budget experiments on \textsc{LTLZinc-Safety-Prop}]{Test set accuracies (mean $\pm$ std over 3 random seeds) on low training budget experiments on \textsc{LTLZinc-Safety-Prop}. Best epoch 0 = only pre-training.}\label{ansya:tab:1epochprop}
\end{table*}

\begin{table*}[!t]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{cccccccc}
			\toprule
			\multirow{2}{*}{\sc Task} & \multirow{2}{*}{\sc Category} & \multirow{2}{*}{\shortstack[c]{\sc Best Epoch}} & \multirow{2}{*}{\shortstack[c]{\sc Average\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Label\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Constraint\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Successor\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Sequence\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}}\\
			& & & & & & & \\
			\midrule
			\multirow{2}{*}{Task 1} & Symbolic & $1$ & $\textbf{0.71} $ {\tiny ($\pm 0.04$)} & $\textbf{0.74} $ {\tiny ($\pm 0.02$)} & $\textbf{0.84} $ {\tiny ($\pm 0.01$)} & $\textbf{0.65} $ {\tiny ($\pm 0.06$)} & $0.60 $ {\tiny ($\pm 0.05$)}\\
			& Neural & $7$ & $0.69 $ {\tiny ($\pm 0.02$)} & $0.72 $ {\tiny ($\pm 0.05$)} & $0.83 $ {\tiny ($\pm 0.03$)} & $0.54 $ {\tiny ($\pm 0.02$)} & $\textbf{0.65} $ {\tiny ($\pm 0.03$)}\\
			\hdashline
			\multirow{2}{*}{Task 2} & Symbolic & $5$ & $0.67 $ {\tiny ($\pm 0.03$)} & $\textbf{0.73} $ {\tiny ($\pm 0.05$)} & $\textbf{0.83} $ {\tiny ($\pm 0.03$)} & $0.60 $ {\tiny ($\pm 0.04$)} & $0.53 $ {\tiny ($\pm 0.01$)}\\
			& Neural & $5$ & $\textbf{0.70} $ {\tiny ($\pm 0.08$)} & $0.70 $ {\tiny ($\pm 0.08$)} & $0.81 $ {\tiny ($\pm 0.05$)} & $\textbf{0.67} $ {\tiny ($\pm 0.08$)} & $\textbf{0.64} $ {\tiny ($\pm 0.12$)}\\
			\hdashline
			\multirow{2}{*}{Task 3} & Symbolic & $0$ & $\textbf{0.63} $ {\tiny ($\pm 0.02$)} & $0.63 $ {\tiny ($\pm 0.05$)} & $0.78 $ {\tiny ($\pm 0.03$)} & $\textbf{0.58} $ {\tiny ($\pm 0.01$)} & $\textbf{0.52} $ {\tiny ($\pm 0.01$)}\\
			& Neural & $6$ & $0.60 $ {\tiny ($\pm 0.01$)} & $\textbf{0.73} $ {\tiny ($\pm 0.01$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $0.35 $ {\tiny ($\pm 0.00$)} & $0.47 $ {\tiny ($\pm 0.06$)}\\
			\hdashline
			\multirow{2}{*}{Task 4} & Symbolic & $3$ & $\textbf{0.67} $ {\tiny ($\pm 0.03$)} & $\textbf{0.73} $ {\tiny ($\pm 0.01$)} & $\textbf{0.83} $ {\tiny ($\pm 0.01$)} & $\textbf{0.56} $ {\tiny ($\pm 0.05$)} & $0.54 $ {\tiny ($\pm 0.03$)}\\
			& Neural & $7$ & $0.66 $ {\tiny ($\pm 0.03$)} & $0.69 $ {\tiny ($\pm 0.02$)} & $0.81 $ {\tiny ($\pm 0.02$)} & $0.54 $ {\tiny ($\pm 0.04$)} & $\textbf{0.58} $ {\tiny ($\pm 0.05$)}\\
			\hdashline
			\multirow{2}{*}{Task 5} & Symbolic & $6$ & $0.78 $ {\tiny ($\pm 0.03$)} & $0.74 $ {\tiny ($\pm 0.03$)} & $0.82 $ {\tiny ($\pm 0.04$)} & $0.79 $ {\tiny ($\pm 0.04$)} & $0.78 $ {\tiny ($\pm 0.02$)}\\
			& Neural & $6$ & $\textbf{0.81} $ {\tiny ($\pm 0.03$)} & $\textbf{0.76} $ {\tiny ($\pm 0.05$)} & $\textbf{0.84} $ {\tiny ($\pm 0.03$)} & $\textbf{0.83} $ {\tiny ($\pm 0.03$)} & $\textbf{0.84} $ {\tiny ($\pm 0.03$)}\\
			\hdashline
			\multirow{2}{*}{Task 6} & Symbolic & $6$ & $\textbf{0.75} $ {\tiny ($\pm 0.03$)} & $0.71 $ {\tiny ($\pm 0.03$)} & $0.84 $ {\tiny ($\pm 0.01$)} & $\textbf{0.72} $ {\tiny ($\pm 0.05$)} & $\textbf{0.74} $ {\tiny ($\pm 0.05$)}\\
			& Neural & $7$ & $0.66 $ {\tiny ($\pm 0.01$)} & $\textbf{0.73} $ {\tiny ($\pm 0.01$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $0.57 $ {\tiny ($\pm 0.05$)} & $0.51 $ {\tiny ($\pm 0.02$)}\\
			\hdashline
			\multirow{2}{*}{Task 7} & Symbolic & $2$ & $\textbf{0.82} $ {\tiny ($\pm 0.02$)} & $0.75 $ {\tiny ($\pm 0.02$)} & $\textbf{0.86} $ {\tiny ($\pm 0.02$)} & $\textbf{0.82} $ {\tiny ($\pm 0.02$)} & $\textbf{0.84} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $7$ & $0.81 $ {\tiny ($\pm 0.01$)} & $\textbf{0.76} $ {\tiny ($\pm 0.00$)} & $0.85 $ {\tiny ($\pm 0.02$)} & $0.81 $ {\tiny ($\pm 0.02$)} & $0.82 $ {\tiny ($\pm 0.01$)}\\
			\hdashline
			\multirow{2}{*}{Task 8} & Symbolic & $2$ & $\textbf{0.72} $ {\tiny ($\pm 0.01$)} & $\textbf{0.75} $ {\tiny ($\pm 0.03$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $\textbf{0.61} $ {\tiny ($\pm 0.01$)} & $0.66 $ {\tiny ($\pm 0.01$)}\\
			& Neural & $7$ & $0.70 $ {\tiny ($\pm 0.02$)} & $0.71 $ {\tiny ($\pm 0.01$)} & $0.81 $ {\tiny ($\pm 0.01$)} & $0.60 $ {\tiny ($\pm 0.03$)} & $\textbf{0.70} $ {\tiny ($\pm 0.06$)}\\
			\hdashline
			\multirow{2}{*}{Task 9} & Symbolic & $7$ & $0.70 $ {\tiny ($\pm 0.06$)} & $0.72 $ {\tiny ($\pm 0.04$)} & $\textbf{0.85} $ {\tiny ($\pm 0.03$)} & $0.58 $ {\tiny ($\pm 0.09$)} & $0.64 $ {\tiny ($\pm 0.09$)}\\
			& Neural & $6$ & $\textbf{0.74} $ {\tiny ($\pm 0.03$)} & $\textbf{0.73} $ {\tiny ($\pm 0.04$)} & $\textbf{0.85} $ {\tiny ($\pm 0.03$)} & $\textbf{0.65} $ {\tiny ($\pm 0.04$)} & $\textbf{0.74} $ {\tiny ($\pm 0.02$)}\\
			\bottomrule
		\end{tabular}
	}
	\caption[High training budget experiments on \textsc{LTLZinc-Safety-Prop}]{Test set accuracies (mean $\pm$ std over 3 random seeds) on high training budget experiments on \textsc{LTLZinc-Safety-Prop}. Best epoch 0 = only pre-training.}\label{ansya:tab:7epochprop}
\end{table*}

Table~\ref{ansya:tab:1epochprop} summarizes the low training budget experiments on \textsc{LTLZinc-Safety-Prop}. 
Label accuracies are often dominated by the Symbolic architecture, albeit often by marginal improvements over the Neural counterpart. The same trend is often observed for Constraint accuracy, however it is important to note, from a training budget perspective, that the Symbolic architecture can achieve these performances by means of pre-training alone (best epoch 0), while the Neural architecture needs an epoch of training on downstream objectives.
Successor accuracy is characterized by mixed trends. Symbolic methods perform better for tasks 1, 3, 5 and 7, while tasks 2, 4 and 9 favor the Neural approach. 
In spite of equivalent upstream \textsc{cc} performance, a Symbolic \textsc{nsp} module is more beneficial for Task 6, while for Task 8 Neural Networks seem favored. These trends roughly correlate with the structure of the Symbolic Finite-state Automaton corresponding to each safety property, with more complex topologies favoring Neural approaches, possibly due to a reduced accumulation of errors at each transition.
%
Table~\ref{ansya:tab:7epochprop} depicts a different scenario for high training budget experiments. The additional training epochs, combined with the simple nature of propositional constraints in \textsc{LTLZinc-Safety-Prop}, allow the Neural architecture to reach higher performances, often on par with the Symbolic one for the first stages of the pipeline. In fact, for Task 3, the Neural approach significantly outperforms the Symbolic one for Label and Constraint accuracies.
With the exception of tasks 3, 5, 6 and 9, Gated Recurrent Units are able to exploit the additional training epochs to learn the next state prediction objective, with performances on par with the Symbolic module.
The challenging nature of tasks 3, 5 and 6 still favors the exploitation of background knowledge, even with the additional training capacity. Gated Recurrent Units can learn to solve Task 9, which is associated with a complex temporal specification, more effectively than the Symbolic module.
In terms of Sequence Classification, Tasks 3 and 4 are challenging for either architecture, with performances close to random guessing.

\paragraph{Results on \textsc{LTLZinc-Safety-FOL}.}
\begin{table*}[!t]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{cccccccc}
			\toprule
			\multirow{2}{*}{\sc Task} & \multirow{2}{*}{\sc Category} & \multirow{2}{*}{\shortstack[c]{\sc Best Epoch}} & \multirow{2}{*}{\shortstack[c]{\sc Average\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Label\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Constraint\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Successor\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Sequence\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}}\\
			& & & & & & & \\
			\midrule
			\multirow{2}{*}{Task 1} & Symbolic & $0$ & $\textbf{0.68} $ {\tiny ($\pm 0.01$)} & $\textbf{0.77} $ {\tiny ($\pm 0.02$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $\textbf{0.56} $ {\tiny ($\pm 0.02$)} & $\textbf{0.56} $ {\tiny ($\pm 0.01$)}\\
			& Neural & $1$ & $0.62 $ {\tiny ($\pm 0.01$)} & $0.75 $ {\tiny ($\pm 0.02$)} & $0.78 $ {\tiny ($\pm 0.01$)} & $0.46 $ {\tiny ($\pm 0.00$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 2} & Symbolic & $0$ & $\textbf{0.70} $ {\tiny ($\pm 0.01$)} & $\textbf{0.76} $ {\tiny ($\pm 0.01$)} & $\textbf{0.86} $ {\tiny ($\pm 0.00$)} & $\textbf{0.60} $ {\tiny ($\pm 0.02$)} & $\textbf{0.58} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $1$ & $0.65 $ {\tiny ($\pm 0.02$)} & $0.75 $ {\tiny ($\pm 0.01$)} & $0.85 $ {\tiny ($\pm 0.01$)} & $0.52 $ {\tiny ($\pm 0.07$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 3} & Symbolic & $0$ & $\textbf{0.64} $ {\tiny ($\pm 0.01$)} & $\textbf{0.77} $ {\tiny ($\pm 0.00$)} & $\textbf{0.86} $ {\tiny ($\pm 0.00$)} & $0.36 $ {\tiny ($\pm 0.01$)} & $\textbf{0.58} $ {\tiny ($\pm 0.04$)}\\
			& Neural & $1$ & $\textbf{0.64} $ {\tiny ($\pm 0.04$)} & $0.74 $ {\tiny ($\pm 0.00$)} & $0.81 $ {\tiny ($\pm 0.01$)} & $\textbf{0.54} $ {\tiny ($\pm 0.12$)} & $0.48 $ {\tiny ($\pm 0.03$)}\\
			\hdashline
			\multirow{2}{*}{Task 4} & Symbolic & $0$ & $\textbf{0.68} $ {\tiny ($\pm 0.01$)} & $\textbf{0.77} $ {\tiny ($\pm 0.01$)} & $\textbf{0.85} $ {\tiny ($\pm 0.00$)} & $\textbf{0.57} $ {\tiny ($\pm 0.03$)} & $\textbf{0.52} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $1$ & $0.61 $ {\tiny ($\pm 0.01$)} & $0.74 $ {\tiny ($\pm 0.02$)} & $0.80 $ {\tiny ($\pm 0.00$)} & $0.39 $ {\tiny ($\pm 0.05$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 5} & Symbolic & $0$ & $\textbf{0.74} $ {\tiny ($\pm 0.01$)} & $\textbf{0.77} $ {\tiny ($\pm 0.01$)} & $\textbf{0.82} $ {\tiny ($\pm 0.01$)} & $\textbf{0.67} $ {\tiny ($\pm 0.01$)} & $\textbf{0.71} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $1$ & $0.62 $ {\tiny ($\pm 0.03$)} & $0.60 $ {\tiny ($\pm 0.07$)} & $0.70 $ {\tiny ($\pm 0.01$)} & $0.57 $ {\tiny ($\pm 0.06$)} & $0.60 $ {\tiny ($\pm 0.04$)}\\
			\hdashline
			\multirow{2}{*}{Task 6} & Symbolic & $0$ & $\textbf{0.68} $ {\tiny ($\pm 0.01$)} & $0.73 $ {\tiny ($\pm 0.01$)} & $\textbf{0.83} $ {\tiny ($\pm 0.00$)} & $\textbf{0.57} $ {\tiny ($\pm 0.03$)} & $\textbf{0.57} $ {\tiny ($\pm 0.02$)}\\
			& Neural & $1$ & $0.61 $ {\tiny ($\pm 0.05$)} & $\textbf{0.75} $ {\tiny ($\pm 0.03$)} & $0.81 $ {\tiny ($\pm 0.01$)} & $0.37 $ {\tiny ($\pm 0.19$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 7} & Symbolic & $0$ & $\textbf{0.73} $ {\tiny ($\pm 0.01$)} & $\textbf{0.78} $ {\tiny ($\pm 0.01$)} & $\textbf{0.81} $ {\tiny ($\pm 0.00$)} & $\textbf{0.66} $ {\tiny ($\pm 0.01$)} & $\textbf{0.69} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $1$ & $0.66 $ {\tiny ($\pm 0.02$)} & $0.69 $ {\tiny ($\pm 0.03$)} & $0.69 $ {\tiny ($\pm 0.03$)} & $0.59 $ {\tiny ($\pm 0.01$)} & $0.65 $ {\tiny ($\pm 0.05$)}\\
			\hdashline
			\multirow{2}{*}{Task 8} & Symbolic & $0$ & $\textbf{0.65} $ {\tiny ($\pm 0.02$)} & $\textbf{0.77} $ {\tiny ($\pm 0.02$)} & $\textbf{0.83} $ {\tiny ($\pm 0.01$)} & $\textbf{0.47} $ {\tiny ($\pm 0.01$)} & $\textbf{0.55} $ {\tiny ($\pm 0.05$)}\\
			& Neural & $1$ & $0.57 $ {\tiny ($\pm 0.03$)} & $0.72 $ {\tiny ($\pm 0.03$)} & $0.75 $ {\tiny ($\pm 0.01$)} & $0.31 $ {\tiny ($\pm 0.08$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 9} & Symbolic & $0$ & $\textbf{0.65} $ {\tiny ($\pm 0.04$)} & $\textbf{0.76} $ {\tiny ($\pm 0.05$)} & $\textbf{0.83} $ {\tiny ($\pm 0.03$)} & $\textbf{0.47} $ {\tiny ($\pm 0.04$)} & $0.56 $ {\tiny ($\pm 0.04$)}\\
			& Neural & $1$ & $0.62 $ {\tiny ($\pm 0.01$)} & $0.71 $ {\tiny ($\pm 0.05$)} & $0.76 $ {\tiny ($\pm 0.02$)} & $0.43 $ {\tiny ($\pm 0.00$)} & $\textbf{0.59} $ {\tiny ($\pm 0.02$)}\\
			\bottomrule
		\end{tabular}
	}
	\caption[Low training budget experiments on \textsc{LTLZinc-Safety-FOL}]{Test set accuracies (mean $\pm$ std over 3 random seeds) on low training budget experiments on \textsc{LTLZinc-Safety-FOL}. Best epoch 0 = only pre-training.}\label{ansya:tab:1epochfol}
\end{table*}

\begin{table*}[!t]
	\centering
	\resizebox{\textwidth}{!}{
		\begin{tabular}{cccccccc}
			\toprule
			\multirow{2}{*}{\sc Task} & \multirow{2}{*}{\sc Category} & \multirow{2}{*}{\shortstack[c]{\sc Best Epoch}} & \multirow{2}{*}{\shortstack[c]{\sc Average\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Label\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Constraint\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Successor\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}} & \multirow{2}{*}{\shortstack[c]{\sc Sequence\\\sc Accuracy}\raisebox{1ex}{\:$\uparrow$}}\\
			& & & & & & & \\
			\midrule
			\multirow{2}{*}{Task 1} & Symbolic & $1$ & $\textbf{0.70} $ {\tiny ($\pm 0.02$)} & $\textbf{0.79} $ {\tiny ($\pm 0.02$)} & $\textbf{0.86} $ {\tiny ($\pm 0.01$)} & $\textbf{0.57} $ {\tiny ($\pm 0.03$)} & $\textbf{0.59} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $5$ & $0.63 $ {\tiny ($\pm 0.00$)} & $0.78 $ {\tiny ($\pm 0.02$)} & $0.78 $ {\tiny ($\pm 0.01$)} & $0.46 $ {\tiny ($\pm 0.00$)} & $0.50 $ {\tiny ($\pm 0.01$)}\\
			\hdashline
			\multirow{2}{*}{Task 2} & Symbolic & $5$ & $\textbf{0.73} $ {\tiny ($\pm 0.04$)} & $0.78 $ {\tiny ($\pm 0.04$)} & $\textbf{0.87} $ {\tiny ($\pm 0.02$)} & $\textbf{0.63} $ {\tiny ($\pm 0.05$)} & $\textbf{0.63} $ {\tiny ($\pm 0.05$)}\\
			& Neural & $5$ & $0.68 $ {\tiny ($\pm 0.00$)} & $\textbf{0.80} $ {\tiny ($\pm 0.01$)} & $0.86 $ {\tiny ($\pm 0.01$)} & $0.57 $ {\tiny ($\pm 0.00$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 3} & Symbolic & $7$ & $\textbf{0.68} $ {\tiny ($\pm 0.01$)} & $\textbf{0.80} $ {\tiny ($\pm 0.02$)} & $\textbf{0.87} $ {\tiny ($\pm 0.01$)} & $0.46 $ {\tiny ($\pm 0.04$)} & $\textbf{0.58} $ {\tiny ($\pm 0.02$)}\\
			& Neural & $6$ & $\textbf{0.68} $ {\tiny ($\pm 0.01$)} & $\textbf{0.80} $ {\tiny ($\pm 0.01$)} & $0.81 $ {\tiny ($\pm 0.01$)} & $\textbf{0.61} $ {\tiny ($\pm 0.00$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 4} & Symbolic & $7$ & $\textbf{0.70} $ {\tiny ($\pm 0.02$)} & $\textbf{0.80} $ {\tiny ($\pm 0.02$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $\textbf{0.59} $ {\tiny ($\pm 0.02$)} & $\textbf{0.57} $ {\tiny ($\pm 0.04$)}\\
			& Neural & $7$ & $0.61 $ {\tiny ($\pm 0.02$)} & $0.78 $ {\tiny ($\pm 0.01$)} & $0.80 $ {\tiny ($\pm 0.01$)} & $0.38 $ {\tiny ($\pm 0.07$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 5} & Symbolic & $2$ & $\textbf{0.76} $ {\tiny ($\pm 0.01$)} & $\textbf{0.80} $ {\tiny ($\pm 0.01$)} & $\textbf{0.83} $ {\tiny ($\pm 0.01$)} & $\textbf{0.69} $ {\tiny ($\pm 0.00$)} & $\textbf{0.71} $ {\tiny ($\pm 0.01$)}\\
			& Neural & $5$ & $0.68 $ {\tiny ($\pm 0.01$)} & $0.76 $ {\tiny ($\pm 0.02$)} & $0.72 $ {\tiny ($\pm 0.01$)} & $0.64 $ {\tiny ($\pm 0.01$)} & $0.59 $ {\tiny ($\pm 0.02$)}\\
			\hdashline
			\multirow{2}{*}{Task 6} & Symbolic & $3$ & $\textbf{0.73} $ {\tiny ($\pm 0.01$)} & $\textbf{0.79} $ {\tiny ($\pm 0.02$)} & $\textbf{0.87} $ {\tiny ($\pm 0.01$)} & $\textbf{0.64} $ {\tiny ($\pm 0.02$)} & $\textbf{0.60} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $6$ & $0.64 $ {\tiny ($\pm 0.01$)} & $\textbf{0.79} $ {\tiny ($\pm 0.02$)} & $0.81 $ {\tiny ($\pm 0.01$)} & $0.47 $ {\tiny ($\pm 0.00$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 7} & Symbolic & $3$ & $\textbf{0.76} $ {\tiny ($\pm 0.02$)} & $\textbf{0.81} $ {\tiny ($\pm 0.02$)} & $\textbf{0.83} $ {\tiny ($\pm 0.02$)} & $\textbf{0.68} $ {\tiny ($\pm 0.02$)} & $\textbf{0.71} $ {\tiny ($\pm 0.03$)}\\
			& Neural & $7$ & $0.71 $ {\tiny ($\pm 0.01$)} & $0.78 $ {\tiny ($\pm 0.00$)} & $0.72 $ {\tiny ($\pm 0.01$)} & $0.63 $ {\tiny ($\pm 0.01$)} & $0.70 $ {\tiny ($\pm 0.05$)}\\
			\hdashline
			\multirow{2}{*}{Task 8} & Symbolic & $3$ & $\textbf{0.66} $ {\tiny ($\pm 0.02$)} & $\textbf{0.80} $ {\tiny ($\pm 0.00$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $\textbf{0.48} $ {\tiny ($\pm 0.00$)} & $\textbf{0.52} $ {\tiny ($\pm 0.07$)}\\
			& Neural & $6$ & $0.59 $ {\tiny ($\pm 0.02$)} & $0.76 $ {\tiny ($\pm 0.06$)} & $0.76 $ {\tiny ($\pm 0.02$)} & $0.35 $ {\tiny ($\pm 0.02$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
			\hdashline
			\multirow{2}{*}{Task 9} & Symbolic & $4$ & $\textbf{0.68} $ {\tiny ($\pm 0.02$)} & $\textbf{0.78} $ {\tiny ($\pm 0.03$)} & $\textbf{0.84} $ {\tiny ($\pm 0.01$)} & $\textbf{0.49} $ {\tiny ($\pm 0.02$)} & $0.60 $ {\tiny ($\pm 0.02$)}\\
			& Neural & $6$ & $0.66 $ {\tiny ($\pm 0.00$)} & $\textbf{0.78} $ {\tiny ($\pm 0.02$)} & $0.76 $ {\tiny ($\pm 0.01$)} & $0.45 $ {\tiny ($\pm 0.00$)} & $\textbf{0.63} $ {\tiny ($\pm 0.02$)}\\
			\bottomrule
		\end{tabular}
	}
	\caption[High training budget experiments on \textsc{LTLZinc-Safety-FOL}]{Test set accuracies (mean $\pm$ std over 3 random seeds) on high training budget experiments on \textsc{LTLZinc-Safety-FOL}. Best epoch 0 = only pre-training.}\label{ansya:tab:7epochfol}
\end{table*}

Table~\ref{ansya:tab:1epochfol} summarizes low training budget experiments for \textsc{LTLZinc-Safety-FOL}. Compared to the propositional case, the increased complexity of constraints produces a different outcome. The Symbolic approach, pre-trained on the \textsc{ic} objective alone (best epoch 0), outperforms, often significantly, the Neural architecture, trained on every objective (best epoch 1) in terms of Label, Constraint and Successor accuracies (with the exception of Task 3, which hints at a failure case of the Symbolic method). In spite of this, Sequence accuracy is often unsatisfactory, with the Neural method often performing as bad as random guessing.
The high training budget experiments, highlighted in Table~\ref{ansya:tab:7epochfol}, confirm the low training budget trends, hinting at the fact that the increased complexity, caused by First-order constraints, cannot be counteracted by an extended training regime alone.


\iffalse
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{table*}[!t]
\centering
\resizebox{\textwidth}{!}{
	\begin{tabular}{cccccccc}
		\toprule
		\sc \multirow{2}{*}{Pattern} & \sc \multirow{2}{*}{Category} & \multirow{2}{*}{\shortstack[c]{\sc Best\\\sc Epoch}}
		& \multirow{2}{*}{\shortstack[c]{\sc Average\\\sc Accuracy}} \raisebox{-1.5ex}{\:$\uparrow$} & \multirow{2}{*}{\shortstack[c]{\sc ic\\\sc Accuracy}} \raisebox{-1.5ex}{\:$\uparrow$} & \multirow{2}{*}{\shortstack[c]{\sc cc\\\sc Accuracy}} \raisebox{-1.5ex}{\:$\uparrow$} & \multirow{2}{*}{\shortstack[c]{\sc nsp\\\sc Accuracy}} \raisebox{-1.5ex}{\:$\uparrow$} & \multirow{2}{*}{\shortstack[c]{\sc sc\\\sc Accuracy}} \raisebox{-1.5ex}{\:$\uparrow$}\\
		& & & & & & & \\
		\midrule
		\multirow{2}{*}{\textsc{Immediate Failure}} & NeSy & $5$ & $\textbf{0.73} $ {\tiny ($\pm 0.04$)} & $0.78 $ {\tiny ($\pm 0.04$)} & $\textbf{0.87} $ {\tiny ($\pm 0.02$)} & $\textbf{0.63} $ {\tiny ($\pm 0.05$)} & $\textbf{0.63} $ {\tiny ($\pm 0.05$)}\\
		& Neural & $5$ & $0.68 $ {\tiny ($\pm 0.00$)} & $\textbf{0.80} $ {\tiny ($\pm 0.01$)} & $0.86 $ {\tiny ($\pm 0.01$)} & $0.57 $ {\tiny ($\pm 0.00$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
		& Random & & $0.36$ & $0.10$ & $0.50$  & $0.33$ & $0.50$ \\
		\midrule
		\multirow{2}{*}{\textsc{Liveness}} & NeSy & $2$ & $\textbf{0.76} $ {\tiny ($\pm 0.01$)} & $\textbf{0.80} $ {\tiny ($\pm 0.01$)} & $\textbf{0.83} $ {\tiny ($\pm 0.01$)} & $\textbf{0.69} $ {\tiny ($\pm 0.00$)} & $\textbf{0.71} $ {\tiny ($\pm 0.01$)}\\
		& Neural & $5$ & $0.68 $ {\tiny ($\pm 0.01$)} & $0.76 $ {\tiny ($\pm 0.02$)} & $0.72 $ {\tiny ($\pm 0.01$)} & $0.64 $ {\tiny ($\pm 0.01$)} & $0.59 $ {\tiny ($\pm 0.02$)}\\
		& Random & & $0.40$ & $0.10$ & $0.50$  & $0.50$ & $0.50$ \\
		\midrule
		\multirow{2}{*}{\textsc{Real-Time Response}} & NeSy & $3$ & $\textbf{0.66} $ {\tiny ($\pm 0.02$)} & $\textbf{0.80} $ {\tiny ($\pm 0.00$)} & $\textbf{0.85} $ {\tiny ($\pm 0.01$)} & $\textbf{0.48} $ {\tiny ($\pm 0.00$)} & $\textbf{0.52} $ {\tiny ($\pm 0.07$)}\\
		& Neural & $6$ & $0.59 $ {\tiny ($\pm 0.02$)} & $0.76 $ {\tiny ($\pm 0.06$)} & $0.76 $ {\tiny ($\pm 0.02$)} & $0.35 $ {\tiny ($\pm 0.02$)} & $0.50 $ {\tiny ($\pm 0.00$)}\\
		& Random & & $0.34$ & $0.10$ & $0.50$  & $0.25$ & $0.50$ \\
		
		\bottomrule
	\end{tabular}
	}
\caption[Test set accuracies on LTLZinc-Safety]{Best results ($mean \pm std$ over 3 replicates) on the proposed tasks. Random indicates a baseline with random predictions at each stage.}
\label{ansya:tab:results}
\end{table*}
\fi

\iffalse
\section{Experiments}\label{ansya:sec:experiments}
Using the LTLZinc framework,\footnote{\url{https://github.com/continual-nesy/LTLZinc}} we generate three tasks, following well-known \LTL patterns for safety-critical applications~\cite{dwyer1998property}. For each task, we assume three spectrogram images $X, Y, Z$, %observed simultaneously at discrete timesteps, 
and the following constraint mapping $\mathcal{C}$:
\begin{align*}
\mathcal{C}\colon\quad&\texttt{p}(X, Y, Z): (X+Y) \equiv Z \mod 10;\\
&\texttt{q}(X, Y, Z): \texttt{all\_different}([X, Y, Z]);\\
&\texttt{r}(X, Y, Z): (X < Y < Z) \vee (X > Y > Z);\\
&\texttt{s}(X, Y, Z): X \not =  Z \wedge (X = Y \vee Y = Z).
\end{align*}
Each task corresponds to a different safety-critical property $\mathcal{F}$:
\begin{description}
\item[\textsc{Immediate Failure}] \texttt{p} is false after \texttt{r}:\\ $\ltlglobally (\texttt{r} \rightarrow \ltlglobally \neg \texttt{p})$;
\item[\textsc{Liveness}] \texttt{s} always follows \texttt{p}:\\ $\ltlglobally (\texttt{p} \rightarrow \ltlfinally \texttt{s})$;
\item[\textsc{Real-Time Response}] \texttt{s} responds to \texttt{p} between \texttt{q} and \texttt{r}:\\ $\ltlglobally ((\texttt{q} \wedge \ltlnext \ltlfinally \texttt{r}) \rightarrow (\texttt{p} \rightarrow (\neg \texttt{r} \ltluntil (\texttt{s} \wedge \neg \texttt{r}))) \ltluntil \texttt{r})$.
\end{description}
%
Datasets contain 1000 sequences (800 train, 100 validation, 100 test samples) of random length between 10 and 25 timesteps. Each timestep is associated with three RGB images sampled from the UrbanSound-Spectrogram dataset,\footnote{\url{https://github.com/mashrin/UrbanSound-Spectrogram}.} resized to fit into an $224\times 224$ image with white background. Images are augmented during training and inference, according to the original ResNet18 transforms~\cite{he2016deep}.
%
The  modular architecture of Figure ~\ref{ltlzinc:fig:pipeline} is initialized in two flavors: \textsc{Neural} (ResNet18, MLP, GRU, red dashed blocks), and \textsc{NeSy} (ResNet18, Deep Problog~\cite{manhaeve2019deepproblog}, NeSy Automaton~\cite{umili2023grounding}, green dashed blocks).
After selecting optimal hyper-parameters (optimizer: Adam, learning rate: $10^{-4}$, MLP:  64 neurons, GRU: 64 hidden units) on a simplified task (the one introduced in Sec.~\ref{ansya:sec:case_study}), we initialize the \textsc{IC} module with one epoch of pre-training on image labels only, then provide supervisions at every stage ($\lambda_{\textsc{CC}} = \lambda_{\textsc{NSP}} = \lambda_{\textsc{SC}} = 1.0, \lambda_\textsc{IC} = 0.1$) for a maximum of 7 epochs. 


Table~\ref{ansya:tab:results} summarizes the results on the test set, evaluated on the best-performing epoch (selected by average accuracy across all modules, measured on the validation set). In general, it can be observed that, although both \textsc{Neural} and \textsc{NeSy} methods reach similar image classification performance, downstream objectives become increasingly difficult for the Neural-only method. Overall, the \textsc{NeSy} method achieves the best performance across all symbolic objectives for every task, even though there is margin for improvement. Knowledge availability can only partially compensate the challenging nature of this setting, as even \textsc{NeSy} approaches struggle with harder formulae. Training behavior (not shown)  indicates that this effect is caused by severe over-fitting at the \textsc{NSP} stage, in spite of good upstream generalization of the \textsc{CC} objective, highlighting optimization challenges of the NeSy Automaton module, in spite of full supervisions available.









\fi