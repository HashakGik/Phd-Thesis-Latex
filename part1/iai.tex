\chapter{Case-based Reasoning}
\label{chap:cbr}
In this chapter we survey the literature of Case-based Reasoning, which represents a significant portion of the Reasoning-by-analogy frameworks in Artificial Intelligence.
%
Human reasoning heavily relies on past experiences and, under a rational decision-process assumption, we can apply a known solution to a given problem, if that solution was valid for a similar instance observed in the past.
This intuition is so strong that it is one of the fundamental principles of many legal systems, taking the name of judicial precedent~\cite{hanna1957role}.

Case-based Reasoning systems (CBR)~\cite{aamodt1994case,schoenborn2021explainable} are an approach to problem solving exploiting past experiences under the assumption that similar problem instances can be addressed by similar solutions.
Case-based Reasoning systems often loop through four phases: \textit{retrieve}, \textit{reuse}, \textit{revise} and \textit{retain}. The first two phases deal with solving the problem: in the former, the system queries a knowledge base in order to find the past experience most similar to the current problem, while the latter phase merges the two problems (past and current) to determine whether the past solution can be used as is, or requires some adaptation to the new problem.
The final two steps of the cycle update the knowledge base: the revise phase generates a feedback which determines whether applying the solution to the new instance has been a success or a failure, and in the retain phase a final decision is made between keeping the new solution, discarding it, merging it with the old one to produce a more general one, and so on.
%
%For a classic review on case-based reasoning, we point the reader towards the work of Aamodt et al.~\cite{aamodt1994case}, for a recent one, with particular emphasis on explainability, we suggest the work of Schoenborn et al.~\cite{schoenborn2021explainable}.
%
%Rudin et al. review only interpretable-by-design approaches concerning the retrieval component of a CBR system, since retrieval is the bottleneck of the entire process, ignoring the other three steps. We partially agree with their decision, because, although the other phases are equally important both in terms of reliability and interpretability of the entire system, we point out that the decision process involving each phase can be made interpretable-by-design by exploiting other techniques, such as constraint injection (section \ref{sec:ciml}).
%For this reason, in this work we will focus only on retrieval techniques, just like Rudin et al., leaving a more nuanced discussion of interpretable-by-design reuse, revise and retain approaches to future work, while pointing out that every component in a truly interpretable-by-design case-based reasoning system must be interpretable as well.
Although transparency is important at every stage, the interpretability bottleneck of a Case-based Reasoner lies within the retrieval component. 

Retrieval systems can be split into (i.) \textit{nearest-neighbor-based} and (ii.) \textit{prototype-based} approaches.
The process of nearest-neighbor search on its own is interpretable-by-design, however two sources of opacity can arise if search is approximated (which is often the case, since an exact search in large databases would be extremely expensive) or if the computed ``similarity'' is non-transparent  (e.g., because it is learned as a black-box function).
On the other hand, prototypes are objects which act as representatives of a given case, as such no distance computations are involved and an archetypal prototype may be constructed in a way which is more representative for the case than any of the training samples. %, this is especially useful when dealing with hierarchical tasks where the training set may not cover every possible combination of cases.
%
%We also note that, in principle, concept bottleneck models (section \ref{sec:sd}) can be used to produce a tabular set of interpretable features which can be exploited as inputs, both for distance (if concepts are continuous) and prototype based approaches (if concepts are discrete or quantized).
Experience Replay (described in Chapter~\ref{chap:continual}, and used extensively throughout the experiments in this thesis) can be considered a form of Retrieval-based search. Chapter~\ref{chap:kandycem} proposes an approach which is inspired by Deep Hashing techniques and employing Concept Embedding Models, introduced in this chapter. Logic disentanglement is a concept encompassing the entire thesis.
\section{Retrieval Based on k-nearest Neighbors}

A distance is any non-negative binary function such that it is symmetric, it satisfies the triangular inequality, and such that it evaluates to zero when applied from an object to itself.
Commonly used distance functions are the ones which can be defined as some $L^p$ norm of the difference between two points in the real vector space, among these, the Euclidean distance ($L^2$) is the most used, in virtue of its intuitive meaning when applied to low-dimensional spaces.
%
The average distance between randomly distributed points in high-dimensional spaces tends to increase with the number of dimensions, this phenomenon, known as the ``curse of dimensionality''~\cite{koppen2000curse}, can cause distance computations to become meaningless from the human perspective. For this reason Case-based Reasoning on high-dimensional spaces, such as images, sequences, and text, cannot directly rely on computing a distance in input space and must, instead, perform some feature extraction steps.
%
%Rudin et al. note that two general trends have arisen in this sense in the domain of retrieval systems, either use an interpretable feature extractor with a sophisticated distance function, or use a complex feature extractor (possibly based on machine learning) together with the Euclidean distance (or another easily interpretable distance), thus implicitly stating that a fully interpretable-by-design approach requires to just replace the least interpretable of the two components.
%
%We argue that this dual formulation may be misleading, since a simple metric used in the wrong domain is no more interpretable than a complex metric (e.g., the Euclidean distance applied to image features has no intuitive meaning, even though it works well in domains such as facial identification), and likewise a simple extractor may not necessarily produce human-understandable features.
%
%Intutively, we can assume that an interpretable (but complex) distance computed on raw features can be considered the "most interpretable" approach, since inputs are unadultered and approximation error can only be injected in one of the two components, instead of tainting both. On the other hand, in specific domains, it can be argued that the extracted features can be manually verified (for example visual features perfectly aligned with human concepts, such as bird's wings or beaks) and so we should assume the interpretability bottleneck to be the distance function, which should be made as simple as possible.
%
%We also note that, although 

\textit{Geometric} distances are the most popular, however, they may not necessarily be the best solution for interpretable retrieval. For instance, \textit{semantic} distances such as the cosine similarity, or the Hamming distance, can boost both performance and interpretability for some domains~\cite{norouzi2012hamming}.
%A third, powerful, option would be given by "statistical" divergences, which quantify the separation between probability distributions (they are not proper distances because they often are non-symmetric) and can provide a solid information-theoretic interpretation, however, we are not aware of any CBR system storing distributions, instead of single datapoints, therefore a discussion on divergence-based approaches is out of scope.
%
Traditional k-nearest neighbors (k-nn) is a non-parametric algorithm, explainable-by-design due to its simplicity, however it can perform poorly, both in terms of computational effort and retrieval accuracy, unless the chosen distance function is semantically meaningful for the task at hand.
The first problem can be solved by ad-hoc methods which trade off exactness of retrieval with execution speed, while the latter can be solved by metric learning, a branch of Machine Learning with the goal of learning distance functions over latent representations.
%
%Approximate nearest neighbor search is a complex and nuanced topic, with an intrinsic difficulty in categorizing methods between interpretable and opaque, therefore we will just scrape the surface of the topic by very briefly discussing one of the approaches, leaving a more organic discussion to future work.

\paragraph{Hashing for Information Retrieval.} A (binary) hash function is any function mapping arbitrary inputs into fixed-length outputs, which can be used to address some memory locations, called buckets. Two objects are said to collide if they are mapped to the same bucket.
While the objective of a cryptographic hash function is to make collisions unpredictable and statistically unlikely, hash functions for information retrieval have the opposite objective of maximizing the likelihood of collisions for similar objects.
In this way, an information retrieval system can be built with a database of objects split into buckets and search can be performed efficiently in two steps: first the hash value of the query object is computed to select a bucket, then a traditional k-nn search is performed to select the target object among those in the bucket (reducing the search space from the entire database, to only the elements inside a single bucket). When retrieval fails, because the target may be on a different bucket, a fallback mechanism is enforced to search the target inside different buckets.

Locality-sensitive Hashing (LSH) an Deep Hashing (DH) are based on two assumptions: the first is that the hash function can be learned from the ``semantics'' of the objects, instead of the raw representation, and the second is that the fallback mechanism will be based on the Hamming distance (i.e., the query and target elements should have a very high probability of being mapped to the same hash, a lower probability of being mapped to two hashes differing by one bit, and so on). This means that the learned functions are somewhat interpretable in the sense that a probabilistic constraint pushes representations to conform to semantic meaning. %Although objectionable, this definition is similar to the one Rudin et al. use for constraint injection models (section \ref{sec:ciml}). We believe that in this context a precise definition would be difficult to provide.
Locality-sensitive Hashing~\cite{jafari2021survey} exploits random mapping of semantically meaningful features to produce semantically meaningful hashes (e.g., by using MinHash permutations over TF-IDF features in natural language processing), since no learning is involved, it could be considered interpretable-by-design, assuming to accept the hash value as interpretation of the data.
Deep Hashing~\cite{luo2020survey} exploits Artificial Neural Networks to learn the hash function, meaning that there is an additional source of opaqueness.%, supervised methods, however, give plenty of room for interpretability. For example, logic disentanglement methods (section \ref{sec:dnn}) can be used to produce hash functions in an interpretable-by-design fashion, while also enforcing constraints on the Hamming distances between objects.
%For in depth surveys, we refer to Jafari et al.~\cite{jafari2021survey} for LSH and Luo et al.~\cite{luo2020survey} for DH.

\paragraph{Metric Learning.} Another way of solving the problems of k-nearest neighbors is to learn the distance function, a branch of Machine Learning known as Metric Learning. 
The Mahalanobis distance~\cite{mahalanobis1936mahalanobis} is a generalization of the Euclidean distance, taking into account data distribution. It intuitively corresponds to applying the Euclidean distance after transforming coordinates in a way such as the dataset becomes normally distributed.
Weinberger et al.~\cite{weinberger2009distance} use a supervised approach to learn a linear transformation of input features, such that the intra-class Mahalanobis distance between objects is minimized, while the inter-class distance is kept above a given margin. Since it uses only linear mappings, this approach can be considered interpretable-by-design.
%
 Deep Metric Learning (DML)~\cite{kaya2019deep} extends the intuitions behind the work of Weinberger et al. to non-linear mappings, by means of Artificial Neural Networks. Salakhutdinov et al.~\cite{salakhutdinov2007learning} are the pioneers of this field and are the first proposing a contrastive approach, where the training objective compares two data points (in terms of Euclidean distance).
 Most of the approaches in Deep Metric Learning inherit the idea of computing the Euclidean distance on top of a non-linear mapping, but although effective, they are often black-box methods, because there is no guarantee on the meaningfulness of learned representations, and in fact many weaknesses, such as dimensional collapse~\cite{jing2021understanding}, where the learned function assigns zero distance to unrelated objects, can make them brittle.
%We believe that future research directions in interpretable case-based reasoning should include interpretable deep metric learning.

\section{Retrieval Based on Prototype Learning}
Prototypes are especially useful for interpretability, because they act as a set of finite representatives of other potentially infinite objects, allowing stereotypical reasoning~\cite{lehmann1998stereotypical}. This type of inference does not guarantee correctness, however, it is way more aligned with human intuition than the form of distance-based reasoning used by k-nearest neighbors approaches.
Another important consequence of sterotypical reasoning is the fact that knowledge gets naturally distilled, from, possibly large or infinite, datasets to smaller sets of prototypes~\cite{bien2011prototype}.

\textit{Clustering} is a family of unsupervised Machine Learning techniques which is concerned with partitioning data points into equivalence classes, called clusters. Since each element inside a given cluster is equivalent, a representative (e.g., the centroid or medoid) is usually chosen for downstream analysis.
Supervised prototypes are based on the same idea, under the assumption that every data point belonging to a class possesses the same characteristics (which are in fact the entities we will refer to as ``concepts'' in section \ref{sec:dnn}). In realistic settings however, not every data point may contain every characteristic related to a class, and the decision boundary may in fact be determined by complex interactions between characteristics. For this reason, it is often necessary to devise a hierarchical structure in terms of characteristics, which can be mirrored by a hierarchical structure of prototypes.

Traditional approaches to prototype-based retrieval can be tracked down to Learning Vector Quantization (LVQ)~\cite{kohonen1995learning}, where a clustering task is solved by learning a quantization function such that all the elements belonging to a given cluster collapse to a single value (acting as prototype), and the work of Bien et al.~\cite{bien2011prototype}, where they define the three characteristics of a ``good'' prototype and solve a classification task by reducing it to an Integer Programming problem. According to their definition, ``good'' prototypes should: (i.) represent as many data points of the target class as possible, (ii.) misrepresent as few data points from other classes as possible, and (iii.) be sparse (each class is associated with few, possibly only one, prototypes).

Modern approaches to prototype-based retrieval follow either the intuition of of Bien et al., where the problem is reframed as an optimization task, or the one of Kohonen, where data points are collapsed to prototypes.
%
Archetypal Analysis~\cite{cutler1994archetypal} is an interpretable-by-design approach which represents (testing) data points as a convex combination of representatives (themselves constructed as convex combinations of the training data). Although interpretable, Archetypal Analysis is inefficient, brittle with respect to outliers and non-local (a data point can be represented as a combination of ``far away'' representatives, which can hinder interpretability). To solve these issues, Wu et al. developed Prototypal Analysis and Prototypal Regression~\cite{wu2017prototypal}.
In these frameworks, locality is enforced by $L^1$ regularization and the expressivity of archetypes is extended by the use of kernel methods.
Both Prototypal and Archetypal Analyses rely on Convex Optimization techniques, therefore they fall in the methods belonging to the first category.

The idea of collapsing data points (the second category) is particularly amenable to be solved by means of Artificial Neural Networks. Moreover, in domains where hierarchical prototypes can be useful, for example in image processing, Neural Networks naturally provide a hierarchical inductive bias, where deeper layers can perform computations related to more ``abstract'' prototypes. %Finally, since ANNs can perform end-to-end tasks, they can exploit prototypical "reasoning" for objectives other than retrieval.
%
Li et al.~\cite{li2018deep} developed a training regime for Neural Networks which is capable of encoding prototypes in the weights of single neurons. In this way, comparisons in the latent space correspond to prototype matching and a decision based on these comparisons is arguably transparent.
They exploit an auto-encoder architecture, where an auxiliary prototype network is attached to the latent bottleneck. The decoder is used for explainability purposes, since it can reconstruct input images from the latent space. Their training loss is composed of traditional classification and reconstruction losses, as well as two interpretability regularization terms: one which pushes prototypes towards meaningful reconstructions and the other pushing every encoded training sample towards the prototypes.
ProtoPNet~\cite{chen2019looks} removes the decoder from the work of Li et al. and replaces the prototype training procedure with a more robust one, which allows for more faithful reconstruction and hierarchical prototypes.
In their work a prototype is no longer an entire training image, instead, their training regime allows to extract patches from the training set, without the need of prototype-level supervision.
They completely remove the decoder from the architecture and replace the latent bottleneck with a Prototype Layer. In this layer, each convolutional kernel is responsible of computing a heat map corresponding to a the region in the input image matching the prototype.
To guarantee interpretability, training is performed in three steps: gradient descent with respect to the classification task, clustering of prototypes and their collapse towards their centroid in the training set, and, finally, Convex Optimization of the last layer, to re-adapt the classifier to new prototypes.
%Since activations in the prototype layer are collapsed to their representatives and since the last layer is linear, the entire approach can be argued to be interpretable-by-design.

Since feature extraction up to the prototype layer is perfomed by a black-box model, extracted prototypes may actually be meaningless. Huang et al.~\cite{huang2022protopnet} perform systematic experiments to better characterize the interpretability properties of ProtoPNets and mitigate some of the identified issues by proposing a shallow-deep feature alignment step.
%
Other extensions of ProtoPNet include sequential applications, such as text~\cite{ming2019interpretable}, and videos~\cite{trinh2021interpretable}.
Rymarczyk et al.~\cite{rymarczyk2021protopshare} also propose pruning strategies useful to merge similar prototypes and reduce the prototype space significantly.
%
ProtoTree~\cite{nauta2021neural} is an interpretable-by-design approach for image classification which fits Probabilistic Decision Tress with prototypes, achieving a hierarchical representation of the decision process.
In their work, a prototype is a tensor trained in the same way as ProtoPNet and capable of being back-propagated to a patch belonging to a training image, and the Decision Tree nodes are traversed with a probability proportional to the presence or absence of a given prototype in the input image. The entire tree acts as a faithful out-of-the-box global explanation, while a single path within it can act as a local explanation for a given sample.
ProtoTrees can be trained by distillation, in order to provide post-hoc explainability of black-box models.

\section{Disentanglement of Neural Representations}
\label{sec:dnn}
Artificial Neural Networks are the archetypal black-box models in modern Artificial Intelligence, and, with their versatility and opaqueness, they position themselves on the exact opposite of the expressivity-explainability spectrum, compared to linear models.
%Among the interpretability issues identified by Rudin et al., disentanglement is of particular importance, to the point that supervised disentanglement and unsupervised disentanglement were assigned a grand challenge each (5 and 6, respectively).
%
Neural networks process data in a strongly distributed fashion, both length-wise (the same input is dispatched to multiple neurons in the next layer) and depth-wise (information is progressively refined as it traverses each layer), however humans tend to better understand centralized information processing and hence prefer to reason in terms of entities with well-defined responsibilities. Along with the huge number of parameters, this ``perspective dissonance'' is the source of opaqueness in Neural Networks behavior. 
Disentangling a Neural Network is the task of enforcing specific responsibilities to specific subsets of the network, by means of inductive biases or constraints. Disentanglement can also be an emergent behavior, arising from particular training regimes and which can be observed in post-hoc explanations (for example specific attention heads in large language models tend to learn specific syntactic/semantic responsibilities~\cite{rogers2020primer}).
%
Rudin et al.~\cite{rudin2022interpretable} define disentanglement as an information-flow constraint, which forces the entirety of ``signal'' related to a semantic concept to be processed by a single neuron. By this definition, a fully disentangled Neural Network can only be realized if the network architecture closely mimics the semantic hierarchy of the task being solved, this limits the applicability to extremely simple problem domains. A common compromise in the literature is reached by imposing disentanglement constraints only to a specific layer, responsible of encoding concepts belonging to a specific level in the semantic hierarchy, this however decomposes a single black-box model into two black-boxes connected by an interpretable layer. 
%In the following we discriminate disentanglement of ANNs with respect to three different types of concepts: semantic, visual and logic. Visual concepts can be exploited for case-based reasoning (section \ref{sec:cbr}), while logic ones strongly relate with constraint injection in machine learning models (section \ref{sec:ciml}), disentangling a model with respect to these types of concepts will also improve the interpretability properties concerning those applications.

\paragraph{Semantic Disentanglement.} Disentanglement with respect to ``semantic concepts'' is a desirable property in terms of human understanding, because it naturally forces the same \textit{divide et impera} approach to problem-solving as logic models. This also allows greater control over training and inference processes, compared to a black-box model: in the former case, disentanglement allows better monitoring learning progress at a fine-grained scale, while in the latter, intervenability allows sample-level explanations, counterfactual reasoning, and human-based correction of mispredictions.
The costs for disentanglement are, however, a dramatic increase in annotation effort for supervised settings, and, in unsupervised scenarios, the inhability of quantifying the "goodness" and the potential lack of human interpretability of discovered concepts. Often, spontaneous unsupervised disentanglement arises as an emergent behavior of Deep Generative Models~\cite{gat2022latent}. % In this paper we focus on supervised disentanglement, leaving the more nuanced discussion of unsupervised methods to future work.
%
Methods for semantic concepts disentanglement in literature broadly categorize into those achieving disentanglement by exploiting (i.) a different training objective and those (ii.) forcing architectural biases.

In the first category, Concept Whitening (CW)~\cite{chen2020concept} is a normalization technique which aligns an arbitrary tensor towards some supervised concepts. A fully connected layer normalized by Concept Whitening produces neurons each responsible for a given concept, while a convolutional layer produces activation maps (which may be pooled into a single value) where each channel is responsible for a given concept. Concept Whitening works by applying a whitening transformation to batches of activations (i.e., computing a linear transformation mapping their covariance matrix into the identity) and then applying the rotation which maximizes the activation of concepts. This process intuitively produces a latent space which is mean-centered and decorrelated with respect to concepts. Training in Concept Whitening networks is not performed with a single loss function and requires instead to optimize alternatively between the task objective and the concept alignment loss. Since it can replace a batch normalization layer, Concept Whitening also allows warm-starting a pre-trained model.

Concept Bottleneck Models (CBMs)~\cite{koh2020concept} are the simplest architectural-bias-based disentanglement method. They work by exploiting supervision, not only on the final layer, responsible for the final decision, but also on an intermediate bottleneck, trained to classify, or regress, the value of concepts important for the final decision. 
In the original formulation, Koh et al. propose three flavors of Concept Bottleneck Models: (i.) \textit{independent}, (ii.) \textit{sequential}, and (iii.) \textit{joint}. 
Independent Concept Bottleneck Models train separately the concept predictor and the final predictor, by passing the true concepts to the final predictor also in case of errors made by the concept predictor (teacher forcing), this flavor achieves the best disentanglement and intervenability, but also suffers from reduced performance.
Sequential Concept Bottleneck Models also train the two predictors separately, but directly pass the concept outputs as the final predictor's input, this flavor achieves intermediate performance, but can be negatively affected by under-represented concepts in the training set.
Joint Concept Bottleneck Models are trained end-to-end, weighing concept and task losses with an hyperparameter, this flavor achieves the worst disentanglement, but also the best performance. 
%
Albeit simple and easy to implement, Concept Bottleneck Models suffer from an heavy accuracy-interpretability trade-off and they fully disentangle the bottleneck layer only if they are trained as independent models~\cite{margeloiu2021concept}.
Concept Embedding Models~\cite{espinosa2022concept}, post-hoc explanation methods~\cite{yuksekgonul2022post} and unsupervised objectives~\cite{sawada2022concept} for Concept Bottleneck Models have been proposed to mitigate such issues.
%
More formally, let $\gD \subseteq \sX \times \sC \times \sY$ be a dataset of perceptual stimuli $\vx \in \sX$ annotated with some task label $y \in \sY$ and intermediate concept annotations $\vc \in \sC$,\footnote{Traditional Concept Bottleneck Models assume Boolean concepts, i.e., $\sC = \sB^{|\sC|}$, but categorical and numerical extensions are possible as well.} then a Concept Bottleneck Model is the function composition $\omega \circ \psi \circ \phi$, 
%
%\TODO{KANDY CEM}
%CBMs~\cite{koh2020concept} popularized the idea of an intermediate layer of human-specified concepts, referred to as concept bottleneck, and used to predict the final output. The set of concept labels $\sC$ is known in advance, thus the meaning of each element is pre-defined. Training examples are composed of task-level and concept-level supervisions, i.e., for each $\vx$ in the training set, the task label $y \in \sY$ and the set of active/not-active concepts $\vc \in \sC$ are fully provided, i.e., datasets are $\gD \subseteq \sX \times \sC \times \sY$.
%The concept bottleneck allows intervention on the values of the predicted concepts within the model. The neural network is responsible of learning a composite function $\omega \circ \psi \circ \phi$, 
where the (i.) \textit{perceptual backbone} $\phi \colon \R^{d} \mapsto \R^{d'}$ computes an initial representation of size $d'$ of the input (for example, it could be a pre-trained backbone), the (ii.) \textit{concept layer} $\psi \colon \R^{d'} \mapsto [0,1]^{|\sC|}$ is the function that predicts concepts, and the (iii.) \textit{task head} $\omega \colon [0,1]^{|\sC|} \mapsto \sY$ yields the final prediction. 
The output of $\psi$ is semantically disentangled by virtue of supervised training, however, in order to further promote interpretability, $\omega$ is often constrained to be interpretable-by-design as well (e.g., a linear classification layer or a symbolic function clearly mapping concepts to task predictions).
%
%Defining $\vy$ as the vector collecting each predicted target $\evy_i$ associated to the $i$-th task, we have:
%\TODO{REWRITE TO REMOVE NUMBERED EQUATIONS AND USE EQUATION* ENV}
%\begin{equation}
%	\vx'=\phi_{\vtheta_1}(\vx),\quad \vc = \psi_{\vtheta_2}(\vx'),\quad \vy = \omega_{\vtheta_3}(\vc).
%	\label{cem:eq:cbm}
%\end{equation}
%
%\TODO{Omega should be interpretable and can be symbolic, interventions replace psi (either to perform sensitivity experiments, or to manually correct test-time errors) ...}
%
%\TODO{A CBM is usually trained with the objective argmin $\vtheta_1$ argmin $\vtheta_2$ argmin $\vtheta_3$ of lambda BCE(c, truec) + lambda CCE(y, truey)}
%
%\TODO{KANDY CEM}
%
%The notion of CBM was generalized and extended in CEMs~\cite{espinosa2022concept}, going beyond the limited expressiveness of a concept-score-only representation. 
Concept Embedding Models~\cite{espinosa2022concept} generalize and extend Concept Bottleneck Models, going beyond the limited expressiveness of a score-only representation of concepts.
In Concept Embedding Models, each concept is associated both with its own score $\evc_j$, and two learnable embeddings, that are mixed by a weighted average controlled by $\evc_j$. In detail, two functions $\rho^{pos}_j$ and $\rho^{neg}_j$ are responsible of computing the aforementioned embeddings for the $j$-th concept, $\rho^{\bullet}_j \colon \mathbb{R}^{d'} \mapsto \mathbb{R}^{e}$, where $e \geq 1$ is the size of each embedding. 
%When predicting the activation scores of the concepts, the following $g$ (Eq.~\ref{cem:eq:cbm}) is exploited in CEMs,
Concept Embedding Models exploit the following $\psi$ when predicting concept activation scores:
\begin{equation*}
	\vc = \psi(\vx') = \upsilon(\rho_0^{pos}(\vx'), \rho_0^{neg}(\vx')) \| \ldots  \| \upsilon(\rho_{|\sC|-1}^{pos}(\vx'), \rho_{|\sC|-1}^{neg}(\vx')),
%	\label{cem:eq:cem1}
\end{equation*} 
where $\upsilon \colon \mathbb{R}^{e} \times \mathbb{R}^{e} \mapsto [0,1]$ is a learnable and differentiable scoring function (usually a single-layer network with sigmoidal activation), and the $\|$ is the vector concatenation operator. Before predicting the task labels, each pair of concept embeddings $(\rho_j^{pos}(\vx'),\rho_j^{neg}(\vx'))$ %$j=1,\ldots,|\gC|$, 
is averaged proportionally the concept activation score $c_j$, yielding $|\sC|$ novel representations, concatenated into vector $\tilde{\vc}$:
\begin{equation*}
	\tilde{\vc} = \evc_0 \cdot \rho_0^{pos}(\vx') + (1 - \evc_0) \cdot \rho_0^{neg}(\vx') \| \dots \| \evc_{|\sC|-1} \cdot \rho_{|\sC|-1}^{pos}(\vx') + (1 - \evc_{|\sC|-1}) \cdot \rho_{|\sC|-1}^{neg}(\vx').
\end{equation*}
Finally, the task-level predictions are computed. However, differently from the case of traditional Concept Bottleneck Models, concepts are not directly represented by their activation scores $\vc$, but by the distributed $\tilde{\vc}$. %We redefine $\omega$ of Eq.~\ref{cem:eq:cbm} as $\omega \colon \mathbb{R}^{|\sC|\cdot e} \mapsto [0,1]^{|\sY|}$ and we get:
%\begin{equation*}
%	\vy = \omega_{\vtheta_3}(\tilde{\vc}).
%	\label{cem:eq:cem2}
%\end{equation*}
This architecture allows the task-predictor $\omega$ (which is now a mapping $\omega: \R^{|\sC|\cdot e} \mapsto \sY$) to have access not only (i.) to the information on how strongly concepts were activated, but also (ii.) to distributed representations of the concepts themselves, since both such types of information were fused into $\tilde{\vc}$. %We will use notation $f_i$ to indicate the component of $f$ associated with the $i$-task predictor. In this paper we focus on CEMs, which represent state-of-the art concept bottleneck models. It is important to remark that most works on CEMs assume access to supervisions at a concept level, differently from the setting of this paper.


\paragraph{Visual Disentanglement.} Part-whole hierarchies are naturally present in images, therefore, visual concepts are somewhat easier to mine (but not necessarily easier to annotate), making visual disentanglement a sought-after property.
Advances in Metric Learning, image retrieval, and quantitative evaluation on the computer vision domain, have spurred a plethora of disentanglement techniques~\cite{zhang2018visual}.

Capsule Neural Networks~\cite{sabour2017dynamic} force groups of neuron (called capsules) to be responsible of specific visual concepts and to produce a vector, such that its magnitude is proportional to the probability of the visual concept being present in the image, and its orientation encodes properties for that concept (e.g., in the MNIST Digits~\cite{lecun1998mnist} dataset, the capsule responsible for the concept "1", can encode position, skewness, stroke thickness, etc.). Learning is performed by means of a Routing-by-agreement algorithm which  ``self-disentangles'' concept properties inside each capsule, makes the output vectors robust with respect to affine transformations, and makes sample-based explanations trivial~\cite{shahroudnejad2018improved}. Deep Capsule Networks are capable of learning hierarchical concepts in a disentangled fashion, at the expenses of an architecture which must closely mimic concept relationships.
Just like Concept Bottleneck Models, Capsule Networks are composed of a black-box backbone, an interpretable component (constituted by one or more capsule layers) and a black-box predictor. GraCapsNet~\cite{gu2021interpretable} make the final predictor interpretable as well, without being forced to use a linear classifier. They work by embedding the capsules' output into a graph and by attributing importance scores to each capsule by means of attention.

Large Language Models (LLMs), built on top of the Transformer architecture~\cite{vaswani2017attention}, tend to self-disentangle their attention heads as an emergent behavior of extreme training regimes~\cite{rogers2020primer}. Authors have argued whether or not attention scores can be considered explanations, and therefore whether a transformer is interpretable-by-design or not (cfr. Jain et al.~\cite{jain2019attention} and Wiegreffe et al.~\cite{wiegreffe2019attention}). Since Vision Transformers are trained under the same regimes, and since they work on inputs composed of sequences of discrete objects (image patches), they allow for the same kind of arguable interpretations. Concept Transformers (CTs)~\cite{rigotti2021attention} try to further strengthen this property, by explicitly enforcing attention scores to follow supervised visual concepts. Concept Transformers achieves this result by learning a concept embedding table (similar to Concept Embedding Models) and by performing cross-attention between the input patches, used as queries, and the concept embeddings, used as key-value pairs. With this architectural constraint, each attention score can be identified as the confidence of a specific concept being present in a specific patch, allowing an off-the-shelf heat map visualization for each concept.

In some cases, Prototype-based Retrieval approaches, are capable of learning part-whole hierarchies between visual concepts, and storing training representatives for each of them. In the case of ProtoPNet~\cite{chen2019looks}, a prototype layer is responsible of learning an unsupervised clustering so that specific neurons are responsible of specific clusters, hence enforcing disentangled representations. Moreover, the training procedure is posed in a way which allows each convolutional kernel to memorize a specific patch from the training set, which acts as representative of the learned cluster.

\paragraph{Logic Disentanglement.} While semantic hierarchies of concepts are difficult to fully annotate and visual hierarchies may be difficult to interpret from a human perspective, logic concepts are simply symbols. As such, they can be manipulated syntactically to produce new symbols and their interpretation will always be valid and unambiguous (whether they map to semantic concepts or not). 
Since most Artificial Neural Networks are trained using stochastic gradient descent, and since Boolean operators have an ill-posed gradient (either zero or undefined), many works exploiting logic concepts, model them as real values between 0 and 1, and assume a probabilistic interpretation (usually frequentist or subjectivist) of the event ``the concept is true'', or a fuzzy semantics. Concept Embedding Models can also be seen as disentangled with respect to logic concepts, because they keep two embedding tables, one for the concepts when they are true, and the other for when they are false, so that a true concept will push the output towards the positive embedding, a false one towards the negative embedding and a value in between will be linearly interpolated between the two.
%
Complete disentanglement of Artificial Neural Networks with respect to logic concepts has been achieved at least as far back as 1994, with Knowledge-based Artificial Neural Networks (KBANNs)~\cite{towell1994knowledge}, where a Neural Network architecture is constructed to perfectly describe a set of Horn clauses, with each neuron's output representing a specific literal, and weights representing the strength of possible implications. After training, an algorithmic procedure is capable of extracting the set of implications above a threshold as an interpretation.

Knowledge-based Artificial Neural Networks belong to Neuro-symbolic Artificial Intelligence (Chapter~\ref{chap:nesyintro}), and they are not the only approach which can be considered disentangled with respect to logic concepts.
%
Semantic-based regularization (SBR)~\cite{diligenti2017semantic} is a popular Neuro-symbolic approach capable of injecting arbitrary fuzzy-logic constraints (Chapter~\ref{chap:ciml}). It works by adding a regularization term to the loss function, composed of a t-norm expression which is a fuzzy representation of a propositional or first-order logic formula. Literals in this formula are functions of input features, output predictions and intermediate neuron activations. It is in principle possible to exploit Semantic-based Regularization for logic disentanglement, provided that each neuron (belonging to a layer for partial disentanglement, or in the entire network for full disentanglement) is associated to a literal, and provided that training time still remains within an acceptable budget when factoring in the evaluation of the (very long) resulting regularization term.
In spite of its simplicity and expressiveness, vanilla Semantic-based Regularization can only provide a training signal, therefore it cannot guarantee disentanglement at inference time. %A way to enforce disentanglement also at inference time with SBR will be presented in section \ref{sec:ntsc}.

The Neural Theorem Prover (NTP)~\cite{rocktaschel2016learning} is a Dynamic Neural Network (a Neural Network which changes its architecture for each input) performing differentiable backward reasoning. Given a knowledge base composed of training facts, rules and a goal, it learns the most useful representation for each atom in the knowledge base, thus combining reasoning capabilities (typical of backward reasoning systems) and robustness to noise (typical of Neural Networks). Each output is computed in four steps: first the traditional Selective Linear Definite resolution algorithm is applied to produce an AND-OR-tree, then the Neural Network's modules are recursively combined to mimic the tree, a soft unification table (matching input tensors and knowledge base symbols, each with a certain confidence) is computed, and finally inputs are propagated through the network to produce the outputs. Since the structure itself of the Neural Theorem Prover is a resolution tree, and since it is possible to extract the most probable unifications from the table, each computation made by the Neural Theorem Prover is highly interpretable and the representation of each symbol is fully disentangled, however the fact that the resolution tree (and therefore the Neural Network size) can quickly grow as the knowledge base size increases, makes NTP application unfeasible for most non-trivial domains. Another issue of the Neural Theorem Prover is training instability, caused by its fuzzy semantics, DeepSoftlog~\cite{maene2024soft} addressed the instability issue by means of probabilistic semantics based on ProbLog~\cite{de2007problog}, and the scalability one, by means of unification tables, which are compact and more-immediately interpretable as well.

Logic Explained Networks (LENs)~\cite{ciravegna2023logic} are Artificial Neural Networks which transform input logic concepts into output concepts, while explicitly providing a First-order Logic formula explaining the transformation. They can be used as classification heads on top of black-boxes, providing an interpretable layer, as fully interpretable stand-alone predictors (possibly by stacking multiple layers in a deep architecture), or, if they are trained by distillation~\cite{hinton2015distilling} on existing black-boxes, as a post-hoc explanation method. Moreover, by replacing the supervised classification objective with unsurpervised maximization of mutual information, Logic Explained Networks can be used as interpretable clustering approaches, capable of discovering logic concepts directly from data. 
A Logic Explained Network module is composed of a set of arbitrary hidden layers, followed by a concept layer trained on a specific objective, based on the type of link between concepts and task (e.g., implication, co-implication, etc.). %encoding IF-rules ($input\_concept \Rightarrow output\_concept$), ONLY-IF-rules ($input\_concept \Leftarrow output\_concept$) or IFF-rules ($input\_concept \Leftrightarrow output\_concept$).
After training, a pruning step enforces sparsity, in order to produce high-quality explanations, as truth tables computed by thresholding inputs and outputs.
%Three flavors of LENs (based on different architectural constraints and pruning strategies) have been proposed, each with a different trade-off between expressive power, post-hoc explainability of a target model and self-interpretability.
\iffalse
$\psi$-networks have the best interpretability, but sacrifice both expressivity and explainability of black-box models, they impose a sigmoid activation to each of the hidden neurons and a node-level pruning of every weight below the top-k: these constraints guarantee disentanglement for each neuron and the dependence from at most k intermediate inputs.
$\mu$-networks, conversely, do not impose any architectural constraint on the hidden layers, but are pruned at the network-level, were the input concepts are globally weighted by importance and the least important ones are removed, this provides a reasonable compromise, by having decent expressivity and interpretability, and the maximum post-hoc explainability.
$ReLU$-networks have the highest expressivity and reasonable post-hoc explainability performance, at the expenses of low interpretability, they impose ReLU activations in the hidden layer and an example-level pruning which "shuts down" a different set of neurons for each input sample.
\fi
\chapter{Constraints and Machine Learning}
\label{chap:ciml}

In this chapter we review the literature at the intersection of Constraint Optimization and Machine Learning, with a particular focus on Constraint Injection in Machine Learning models.
%Rudin et al. define interpretable-by-design models enforcing mathematical constraints (challenge 8 of their work) as models which can be "expected" to follow a set of known constraints.
In Chapter~\ref{chap:ansya} we approximate a known intractable task by reducing it to an easier to solve one. The injection of combinatorial constraints is relevant throughout the thesis.
\section{Efficient Approximation of Known Functions} Physics laws are often described by Ordinary Differential Equations (ODEs) or Partial Differential Equations (PDEs), however exact solutions are often either computationally expensive or unknown.
%
Physics-informed Neural Networks (PINNs)~\cite{raissi2019physics} exploit the universal approximation property of Artificial Neural Networks, to approximate Partial Differential Equations in an efficient and highly parallelizable fashion. They are trained in a supervised fashion on a dataset of initial and boundary conditions, and as such they do not suffer from annotation bottlenecks or lack of generalization guarantees, affecting traditional Neural Networks.
Physics-informed Neural Networks minimize the mean squared error between the gradient of the learned function and the Partial Differential Equation applied to the function itself.
Since Neural Networks are trained by stochastic gradient descent (or its variants), they are differentiable by construction and the derivative at each data point is easily accessible by means of back-propagation.
The Physics-informed Neural Network training objective can also be used as a regularization term, when learning tasks in which it is important to enforce constraints in the form of Partial Differential Equations.

%Although Rudin et al. consider PINNs interpretable-by-design, by virtue of the fact that they explicitly approximate a known function (instead of the traditional ML process in which the learned function is unknown a priori), we argue that this is form of "transparency" is too weak and it should be handled with care, moreover, when using the PINN objective as a regularization term, the resulting model is no longer guaranteed to follow the governing equations at inference time (a problem we will discuss more in detail below).
%
Gaussian Processes (GPs)~\cite{mackay1998introduction} are a collection of infinite random variables such that every finite subset follows a multivariate normal distribution. Since Gaussian Processes can be fully parametrized by their second order statistics, they are a popular generative Machine Learning framework for probabilistic regression.
Neural-net-induced Gaussian Processes (NNGPs)~\cite{pang2019neural} exploit the fact that covariance kernels of a Gaussian Process can be represented by Rectified Linear Unit Artificial Neural Networks, to approximate Partial Differential Equations better than Physics-informed Neural Networks and vanilla Gaussian Processes.
%In general GPs can be considered black-box models, however Rudin et al. note that, by imposing constraints on how samples are drawn, they can be considered interpretable-by-design, in the same sense as PINNs, that is they can be trained as efficient function approximators of PDEs, and as such their behavior is expected to follow the PDEs.

\section{Learning Combinatorial Optimization} Rational decisions are nearly universally the result of an optimization process, where costs and benefits are weighted in order to select the ``best'' choice. Machine Learning models are trained by minimizing a loss function with respect to some parameters, however, once trained, a traditional Machine Learning model cannot perform an optimization process by itself.
%
Combinatorial Optimization (CO)~\cite{schrijver2005history} is the subfield of optimization with the goal of extracting the best element from a finite set, and therefore it can be considered a form of symbolic reasoning mimicking the process of rational decision.
Combinatorial Optimization is extremely powerful and can provide the optimal solution for a given problem (modulo heuristic approximations for tractability) in a transparent and understandable way, thus making approaches interpretable-by-design, however, Combinatorial Optimization suffers from extreme computational costs (often relying on \textsc{NP-hard} algorithms) and lack of flexibility, which makes ``learning'' impossible.
On the other hand, Machine Learning approaches are function approximators which cannot provide any guarantee of optimality and are often difficult to explain, but are typically extremely fast (at inference time) and very flexible, since they can learn better approximations from data.
%
%We advocate that a tight integration between CO and ML will overcome the limits of either technique and produce methods which are both optimal and capable of learning from data, while being fast and interpretable.
In this section we focus on the integration of Combinatorial Optimization and Artificial Neural Networks~\cite{kotary2021end}, i.e., we focus on the subset of Neuro-Symbolic Artificial Intelligence concerned with optimization.
%
%Marra et al.~\cite{marra2024statistical} wrote an in-depth survey of NeSy AI in general, we point the reader to Kotary et al. for a survey specifically aimed at the intersection between CO and NeSy AI.

Interpretable-by-design models enforcing mathematical constraints can be grouped in two categories: (i.) models interacting with a Combinatorial Optimization process at inference time, and (ii.) models made interpretable by enforcing constraints at training time.
These two, apparently distant, families are in fact closely related from the decision-making perspective.
Human decision-making can be aided by predictive analytics tools, which regress a future event from a known initial configuration, and prescriptive analytics tools, determining the proper action to take in a given situation.
%
In the case of predictive analytics there is often the need for high accuracy and speed (since they often aid decision-making with what-if analyses, usually requiring many predictions from different starting conditions), as such Machine Learning models can often achieve great results, but usually need to be constrained to follow certain knowledge-driven behaviors to avoid drifts.
On the other hand, prescriptive analytics need to mimic the rational process of a human agent, and, as such, they need to output a decision which optimizes some criteria. Since decisions are drawn from a finite set, combinatorial optimization is arguably the best toolbox for this task. However, knowledge-elicitation from experts may be difficult or impossible in some domain, thus the integration of Machine Learning and Combinatorial Optimization is desirable for prescriptive analytics as well.
%
%Predictive analytics methods based on (softly) constrained machine learning models are interpretable-by-design, according to the definition provided by Rudin et al., since they are "trained explicitly to follow a set of constraints", in our opinion, however, this is a weak definition which does not provide any guarantee at inference time, i.e., the most important and delicate part of a machine learning model life cycle, since it is the moment when generalization issues can arise and when critical failures can occur.
Predictive analytics methods based on constrained Machine Learning at training time do not typically offer any guarantee during inference, however they tend to be significantly faster than exact optimization, making them the only viable approach for specific use-cases. %, however, these methods still inherit from black-box models their efficiency and are often the only viable solution. 
For example, in real-time or reactive applications, a full-blown optimization algorithm cannot be executed within the allotted time, and correctness, and accuracy must often be traded off with responsiveness. 
%We do not discourage the use of softly constrained ML models, however, we strongly advice to take proper care and precautions, since their trustworthiness is only slightly above their "pure" (possibly black-box) counterpart.
%
Hard constraints are more amenable to prescriptive analytics, but also situations in which strong guarantees are required for predictive analytics as well, for example when applied to safety critical or ethically ``challenging'' domains.
%
%Finally, it is important to note how the line between soft and hard constraints is blurry: hard-constraining techniques can be used only at training time, leaving inference time "unconstrained" to improve speed, or, vice versa, some soft-constraining techniques can be applied as functions which transform a constraint-violating input into a constraint-enforced output, also at inference time with limited computational overhead. In the following sections, we will cover these nuances where appropriate.
%
%We are going to cover only methods which exploit ANNs, so that we can make a parallel with section \ref{sec:dnn} and treat interpretability with respect to constraints as a form of disentanglement.
%
%Before moving to a survey on existing constraint-injection methods, we give a few recent examples on applications in the health domain, to showcase the social usefulness of constrained models for both predictive and prescriptive analytics.
%The recent SARS-Cov-2 pandemics elicited the need for quick extremely-high stake decisions which require a complex and precise analysis of huge amounts of data. In such a complex and emergential situation, human decision-makers did and still do require support from robust analytics, both predictive and prescriptive, to minimize the losses in human lives and the spread of the virus.
%
%Compartmental models~\cite{brauer2008compartmental} are a popular mathematical framework for epidemics prediction, however, traditional methods can be brittle with respect to errors in parameters estimation. While statistical approaches such as Bayesian melding~\cite{zhong2015latent} can partially mitigate the issue, data-driven approaches tend to be more effective.
%Using approaches such as regressing compartmental model variables by means of ANNs (e.g., Jo et al.~\cite{jo2020analysis}), or, vice versa, encoding compartmental model constraint on ANNs (e.g., the predictor of Lozano et al.~\cite{lozano2021open}), both qualify as constrained ML models for predictive analytics.
%
%SARS-Cov-2-related prescriptive models usually involve complex decisions such as geographic allocation of intensive care units or lock-down scheduling, and are a much more delicate task which requires much stronger constraints and interpretability, given the heavy toll in human lives associated with wrong decisions.
%Miikulainen et al.~\cite{miikkulainen2021prediction} use a predict-then-optimize approach (see section \ref{sec:ico}), where a compartmental model predictor feeds a population-based optimization (neural architecture search~\cite{liu2021survey}) of non-pharmaceutical prescriptions (ranging from the use of facial masks, to restictions on international travels).
%Lozano et al.~\cite{lozano2021open}, the winners of the XPrize Pandemic Response Challenge, embed constraints based on confinement interventions into a compartmental-model-based predictor (making it interpretable, according to Rudin et al. definition), and use gradient boosted trees for the non-pharmaceutical intervention prescriptor, hence making the entire pipeline interpretable-by-design both for prediction and prescription.
%
%We cannot stress enough the importance of safety guarantees in high-stakes domains, such as a global pandemic, and we feel that, although interpretable-by-design and invaluable information-processing techniques, these methods are far from being completely reliable. We hope that proper care is taken by decision-makers when including these methods in the chain of trust, possibly and hopefully by always filtering automatic decisions with rational judgement.

\paragraph{Hard Constraints.}
%\label{sec:ico}
%
The most straight-forward way of enforcing Combinatorial Optimization constraints in a decision-making setting is the Predict-then-optimize (PTO)~\cite{el2019generalization} framework, a two-stage approach in which a predictor produces the inputs for a combinatorial optimizer, which then transforms it to output the final decision. Since the two components are decoupled, Predict-then-optimize does not allow any feedback, which could help the predictor achieve better convergence, however, since the predictor outputs have a downstream symbolic interpretation and since the optimizer is transparent, a Predict-then-optimize approach is interpretable-by-design in the sense that data flow is disentangled at least for the last layer of the predictor, in a way similar to Concept Bottleneck Models.
%
Problems solvable with Predict-then-optimize approaches are characterized by well-defined decision variables and feasible regions, but costs with an unknown dependency with respect to input features, for this reason predictors are usually Machine Learning models, capable of learning such costs from training data.
%
Smart Predict-then-optimize (SPO)~\cite{elmachtoub2022smart} improves Predict-then-optimize by introducing a convex loss function penalizing those mispredictions which induce decision errors. This loss can be used either as a regularization term for the Machine Learning component of Predict-then-optimize, or to directly embed the decision component in the predictor.% Although improving performance, SPO does not improve the interpretability properties of vanilla predict-then-optimize.
%
Mulamba et al.~\cite{mulamba2020contrastive} propose a Predict-then-optimize approach based on contrastive learning, where non-optimal solutions of the optimization problem are used as negative samples. A caching mechanism significantly reduces invocations of the solver, hence reducing the overall computational burden.
%This approach is interpretable according to the definition of Rudin et al., in the sense that the metric space learned is "well-behaved" enough to "follow" soft constraints also at inference time, but from our perspective it has a lower interpretability than other methods (high-dimensional visualization is complex enough that Rudin et al. dedicated challenge 7 entirely to this task).

One important issue of Predict-then-optimize is that feeding a solver with predictions requires a large amount of calls to the Machine Learning model, which may slow down the entire process, already slow due to the optimization algorithms and the inability of exploiting pruning strategies (since values are unknown before calling the predictor). Another important issue is the fact that, without Smart Predict-then-optimize, no information flows backwards to the predictor, decoupling the learning process from the optimization one, thereby missing the possibility of a more targeted learning.
Empirical Model Learning (EML)~\cite{lombardi2017empirical} is a framework which addresses these issues by tightly integrating the optimization and prediction models in a way which allows both domain pruning and backflow of information.
Empirical Model Learning is a generic framework combining arbitrary predictors and optimizers, with the only assumption of being able to encode a set of constraints linking input and output variables for each component of the predictor.\footnote{e.g., Empirical Model Learning can combine ReLU-only Artificial Neural Networks and Satisfiability Modulo Theories by defining, for each neuron $y = (\displaystyle\sum_i \eva_i \evx_i + b)^+$, a constraint in the form $Y \geq 0 \wedge Y = A_0 X_0 + \dots + A_n X_n + B$, where each literal is an optimization variable.}
%
In their original work, Lombardi et al. formalize the framework for Artificial Neural Networks or Decision Trees (DTs) as predictors, and Local Search (LS), Mixed Integer Non-linear Programming (MINLP), Constraint Satisfaction (CS) or Satisfiability Modulo Theories (SMT) as optimization frameworks, however the basic principle can be generalized to other combinations.
The predictor parameters are trained during a learning phase and their values are used to seed the optimization variables with initial guesses, and then the optimizer can find a feasible solution faster than unguided optimization. Unlike Predict-then-optimize, Machine Learning parameters can be further tuned by the optimization step, and in theory, the process can be iterated, providing backward feedback to the predictor as well.
%The resulting optimization problem is interpretable-by-design, since it guarantees the enforcement of domain specific constraints, however it may not be human-understandable, since encoding the predictor may produce a rather large combinatorial problem specification.
\iffalse
Just as an example, here we describe how EML can be used to combine feed-forward ANNs and CS, a similar idea applies to the other combinations of optimizers and predictors as well. A single neuron is a non-linear function applied to the weighted sum of its inputs, therefore at any given moment, the constraints $Y = \phi(Y')$ and $Y' = b + \displaystyle\sum_{i}(w_i \cdot X_i)$ must hold. If the non-linearity function $\phi$ is already encoded in the CS background theory, the ANN predictor can be directly encoded as a set of constraints on inputs $X_i$ and outputs $Y$ (properly connecting variables to mimic the network architecture) and initializing constants $b$ and $w_i$ with values learned during training. In case backflow of information is allowed, we can simply make the parameters $B$ and $W_i$ variables as well, with initial guesses $b$ and $w_i$ set to be the ones learned.
When the non-linearity function is not implemented by the CS solver, two bound consistency filters must be implemented as well, to enforce efficient pruning: an update on $y'$ triggers $max\_y = min(y, \phi(y'))$ and an update on $y$ triggers $max\_y' = min(y', \phi^{-1}(y))$. If the non-linearity is non-invertible, a value for the second filter can still be found by brute-forcing, at the expenses of efficiency.
\fi

A more refined way of providing feedback backwards, from the optimizer to the predictor, requires a gradient flow. This can be achieved, for instance, by embedding a Constraint Optimization approach inside a Machine Learning one. 
From the interpretability perspective, this means that data flow can be constrained to follow arbitrary paths and achieve disentanglement also at levels other than the last (possibly achieving full disentanglement of neural networks, if each neural layer is intertwined with optimization layers, in a way similar to Logic Explained Networks).
%
CVX Layers~\cite{agrawal2019differentiable} encode disciplined convex programs in a fully-differentiable fashion, bridging the effectiveness of convex optimization solvers and the execution time of Neural Networks. Since the mapping from neural input to optimization variables and back to neural output is given by two affine transformations, interpretation of the neural layers surrounding a CVX Layer is trivial.
The optimization problem for each layer is specified in a human-understandable domain-specific language, making the approach even more amenable to interpretation.
%
Pogan\v{c}i\'c et al.~\cite{poganvcic2020differentiation} propose a simple and efficient method to compute the backward pass through any combinatorial solver with linear objective functions, allowing the implementation of an arbitrary symbolic layer for Artificial Neural Networks.
In particular, the forward pass is computed by invoking the combinatorial solver on the predicted inputs, just like traditional Predict-then-optimize methods, then an input perturbation is computed by taking a step, controlled by an hyper-parameter, towards the direction pointed by the gradient of the objective function, finally the optimizer is invoked again and the gradient for the backward pass is computed as the finite difference between the perturbed input and the original one.
%They showcase the effectiveness and flexibility of their approach on mixed integer programming solvers, Blossom  and Dijkstra algorithms.
%
SMTLayers~\cite{fredrikson2023learning} are yet another interpretable layer, which embeds Satisfiability Modulo Theories (SMTs) or MaxSAT constraints into Neural Networks. The constraints are user-defined, further increasing the interpretability of the system, at the expenses of expressivity.
An SMTLayer is a non-trainable layer, fully parametrized by the set of user-defined constraints, and can be used similarly to a Logic Explained Network, where both inputs and outputs are groundings of symbols appearing in the constraints.
In the forward pass, input logits are thresholded by a sign non-linearity, the solver is called and outputs are provided to the next layer. In the case of SMT solvers, hard assignments are returned, for MaxSAT solvers, the output provides a softmaxed probability distribution on symbols.
To solve the ill-posed gradient (the sign threshold has gradient zero almost everywhere), the backward pass of SMTLayers exploits input perturbations, by computing a counterfactual input, and the gradient of the binary cross-entropy between the original input and the counterfactual is returned as training signal.
SATNet~\cite{wang2019satnet} layers are a similar approach in which MaxSAT constraints are instead learned.

\paragraph{Soft Constraints.}
%\label{sec:ntsc}
Sometimes it is either infeasible or inefficient to enforce hard constraints on Machine Learning models, for instance in real-time applications or node-level inference, time or computational budgets, respectively, may be insufficient to allow the luxury of a full-blown optimization. In these circumstances, ``guiding'' the learning process may still be desirable, even though strong guarantees at inference time are lost. %Members of this family of constraint-injection methods are still interpretable-by-design according to the definition given by Rudin et al., however we advocate that these methods should be considered black-boxes and treated as such when reasoning about safety and high-stakes decisions.
%
If the learning task is framed as a prediction problem, the predictors of Predict-then-optimize approaches can fall into this category as well, in particular Smart Predict-then-optimize loss and the noise contrastive loss of Mulamba et al. can be used to impose soft constraints (computed only at training time by the optimizer) on the predictor.

We already mentioned how Semantic-based Regularization (SBR)~\cite{diligenti2017semantic} can be exploited to guide logic disentanglement (Chapter~\ref{chap:cbr}), here we describe the framework more in detail. Semantic-based Regularization exploits triangular norms (t-norms) to construct a differentiable algebraic formula, which is the fuzzy equivalent of a logic specification. A t-norm is any binary function defined on the real unit interval ($f: [0, 1]\times[0, 1]\mapsto[0, 1]$), which is commutative, associative and monotone, and such that the element $1$ acts as neutral. A fuzzy propositional logic framework replaces the logic conjunction with a t-norm, and, since negation can always be defined as $\neg \texttt{p} \doteq 1 - p$ and equivalence axioms hold also in a fuzzy setting, every other operator can be automatically defined. In First-order Logic, at least one quantifier (existential or universal) must be defined as well, as an aggregation function, the other one can be derived by De-Morgan rule.
%The most common used fuzzy logics for SBR are based on the product t-norm ($A \wedge B \doteq A \cdot B$) and the minimum t-norm ($A \wedge B \doteq min(A, B)$, also known as G\"odel t-norm). The universal quantifier in a FOL fuzzy logic is usually defined as the minimum over every possible grounding of the quantified variable in the t-norm expression ($\forall x P(x) \doteq \displaystyle\min_{x_i \in \mathcal{X}}\mathcal{P}(x_i)$). Since validity of equivalence axioms is only guaranteed at boundary values ($0$ and $1$), some fuzzy logic systems prefer to define t-norms for the entire set of binary operators.
An expression using only t-norms, quantifiers and the negation is guaranteed to be differentiable, and as such amenable to be used during the training process of a Neural Network. If atomic propositions appearing in the expression are bounded in the range $[0, 1]$, the expression can be evaluated to a number in the same range and have a fuzzy logic interpretation.
A neuron or layer output can be constrained in such range by using a suitable activation function (e.g., sigmoid or softmax) and an input feature can always be converted into Boolean predicates meaningful for the application domain (a one-hot encoding is already a set of binary values, a numeric feature can be thresholded, etc.).
Semantic-based Regularization is simply the application of a regularization term, which provides an opposing gradient when a t-norm expression is false, to any learning loss: $\gL(\vtheta, \vx, \vy) = \gL_{task}(\vtheta, \vx, \ vy) + \lambda_{SBR} (1 - \gL_{SBR}(\vtheta, \vx, \vy))$, where $\lambda_{SBR}$ is an hyper-parameter trades off between task adherence and constraint importance. This is similar to how Physics-informed Neural Networks enforce adherence to Partial Differential Equations during training.
%
%From our perspective, SBR used in this way make an ANN no more interpretable than a black-box, however t-norm expressions can also be used as interpretable layers inside the ANN, in a way similar to logic explained networks. For instance, in product t-norm, a conjunction layer can be implemented as element-wise multiplication and an existential quantifier layer can be implemented as max-pooling. In this way, if the SBR-block inputs are constrained in the $[0, 1]$ range, the corresponding neurons can be disentangled as responsible for a specific atom of the formula and the output of the block can be interpreted deterministically as the application of a logic expression to those inputs. A consequence of using t-norm expressions as layers is the guarantee of hard-enforcing the logic constraint they represent also at inference time, like combinatorial optimization methods based on differentiable layers.
%Logic Tensor Networks~\cite{badreddine2022logic} extend Semantic-based Regularization, by allowing t-norm expressions to appear in arbitrary Neural Networks layers, thus allowing (fuzzy) constraints to be enforced also at inference time, when the loss function is no longer available.

The Semantic Loss (SL) function~\cite{xu2018semantic} is conceptually similar to Semantic-based Regularization, but constrains learning by means of a semantically sound encoding of a propositional logic formula. Although it cannot be generalized to First-order Logic, Semantic Loss is provably the only loss function , up to a multiplicative constant, satisfying a set of mathematical properties useful for correctness, but also interpretability, namely: monotonicity ($\varphi \models \varrho$ implies $\forall X \gL_\varphi(X) \geq \gL_\varrho(X)$), identity (the loss for a deterministic vector is the vector itself), label-literal correspondence (the loss of an atomic formula is proportional to the negative log-probability of the atom being true), and differentiability.
Akin to Semantic-based Regularization, the Semantic Loss can be applied to any value normalized in the $[0, 1]$ interval, therefore it can enforce constraints related to inputs, outputs or intermediate layers.
Given a constraint in the form of a propositional formula, it is straightforward to derive its truth table and determine which assignments satisfy the formula. The Semantic Loss function is built directly on top of satisfiable assignments, abstracting from the formula itself, but at the cost of an exponential number of terms, with respect to the number of atoms.\footnote{In contrast, Propositional Semantic-based Regularization directly encodes the formula, which is evaluated once for each input.}
%If $x$ is an assignment which satisfies formula $\alpha$ and $p_i$ is the probability of literal $X_i$ being true, then the semantic loss for $\alpha$ is $\mathcal{L}(\alpha, P) = - log(\displaystyle\sum_{x \models \alpha} \displaystyle\prod_{i: x \models X_i}p_i \displaystyle\prod_{i: x \models \neg X_i}(1 - p_i))$, which is the negative log-likelihood of the products of the probablility of positive literals being true and negative literals being false for every possible assignment satisfying the formula. This is proportional to the negative log-likelihood of the formula being true, given the probabilities of each assignment.
%Unlike t-norms, such probabilistic definition remains intuitive not only at the boundary values of 0 and 1, but also for each intermediate value. In our opinion this makes SL "more" interpretable than SBR, while still suffering from the inhability of imposing hard constraints at inference time. On the other hand, SL is less expressive than SBR, since it cannot encode quantifiers.

Semantic-based Regularization can encode First-order Logic formulas, however its practical application may be unfeasible, because quantified variables must be expanded combinatorially to every possible value they can assume, therefore more efficient soft-constraining techniques are needed in relational settings.
For example, in Relational Learning, it is often important to model implication constraints between relations in the form $\forall T: r_1(T) \rightarrow r_2(T)$, where $T$ is an n-ary tuple. The universal quantifier is implicitly expanded n times, and then evaluated for each possible value in the domain. Furthermore, Relational Learning usually is applied to large knowledge bases, therefore the t-norm expression for a single constraint would be intractably large. In the case of infinite domains (i.e., relations defined as subsets of $\sN^n$ or, worse, $\R^n$), expanding the t-norm expression and computing the regularization term with Semantic-based Regularization would be impossible.
%
Lifted-rule Injection~\cite{demeester2016lifted} solves this issue with a specialized loss aimed at enforcing implication constraints on the latent representation of learned relations.
By assuming latent representations of tuples and relations which encourage alignment for tuples belonging to the relation and orthogonality for tuples not belonging to the relation, their dot product $r^Tt$ can be interpreted as a compatibility score~\cite{riedel2013relation}.% Starting from this intuition, it is possible to rank two different relations applied to the same tuple, i.e., if $t \in R_1$ and $t \not\in R_2$, then $r_1^Tt \leq r_2^Tt$, because $t$ is more "compatible" with $R_1$ than it is with $R_2$.
By encoding relations in this way, training for the primary task can be performed with any Metric Learning loss, by considering the training samples belonging to each relation as positives and randomly sampled tuples as negatives, possibly with $L^2$ regularization on the latent representations of both tuples and relations to avoid overfitting.
Noting that the t-norm for implication can be defined as a difference, it is possible to define an additional term to the loss function. This term is still intractable because it depends on every possible tuple, however the lifted loss (which is tuple-independent) can be defined as a convex upper bound for this term.
%Lifted-rule Injection suffers from the same limited interpretability as SBR and SL, however, given the highly specialized way in which constraints are injected, we reckon it may be possible to devise post-hoc explainability techniques specifically targeting relation "compatibility" and lifted-rule constraints, which are more informative for the relational learning domain than general-purpose post-hoc methods.
