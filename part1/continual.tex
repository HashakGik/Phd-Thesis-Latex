\chapter{Continual Learning}
\label{chap:continual}

In this chapter we briefly describe Continual Learning~\cite{de2021continual}, the subset of Machine Learning concerning learning over time, and non-independent and identically distributed (non-i.i.d.) regimes. Concepts described herein are relevant for the entire thesis.
The main assumptions of learning over time are the facts that there exists an order relation between experiences observed by an agent, and that recalling past experiences is an expensive process mediated by a limited-size memory. In this setting, learning is characterized by a recency bias, with experiences closer in time having a stronger effect on the agent, as is widely known in the case of Neural Networks~\cite{PARISI201954}.
If such bias is not adequately counteracted, the agent can be subject to a phenomenon known as \textit{catastrophic forgetting}: performance on past experiences drops dramatically, as soon as the agent attempts to learn on a new experience.
More generally, an agent with limited learning capabilities and finite-size memory is usually affected by the \textit{stability-plasticity trade-off} when it is required to learn different objectives at different times: being able to preserve old knowledge negatively affects the ability to assimilate new experiences, and vice versa. 
%%%%
Works covering learning over time can be found in the context of different fields, focusing on different aspects of the learning problem, which is known to introduce several challenges~\cite{casoni2024pitfalls}. Foundational aspects at the intersection with optimal control have been recently studied by Melacci et al.~\cite{melacci2024unified}, proposing a novel learning framework called Hamiltonian Learning. These studies point toward the direction of re-considering state-space models to make spatio-temporally local learning feasible~\cite{tiezzi2025back}. The role of foundation models is multifold in learning over time. On one hand, they offer useful backbones that can be kept frozen~\cite{zhou2024continual}. On the other hand, they can be slowly fine-tuned~\cite{zhang2023slca} or patched with the introduction of specific adapters~\cite{graziuso2024task}, making them a valid starting point to develop solutions that continuously adapt over time.

%%%%
\paragraph{Incremental Learning.}
Traditional settings in Continual Learning research are based on the Incremental Learning framework~\cite{de2021continual}. Within this framework, a learning agent has access to a sequence of datasets $\mathcal{D}^{(t)} \subseteq \sX^{(t)} \times \sY^{(t)}$, observed one at the time,\footnote{In the Continual Learning literature a single dataset $\mathcal{D}^{(t)}$ is called ``task''. To avoid confusion with the notion of task in Neuro-symbolic Artificial Intelligence, throughout this work we will use the terms ``episode'' or ``learning experience''.} for each time-step $0 \leq t < T$. Samples within each $\mathcal{D}^{(t)}$ can be shuffled and drawn in an i.i.d. fashion, and trained until convergence (i.e., within each experience, a single dataset can be processed following traditional Deep Learning frameworks).
The goal of the agent within an Incremental Learning setting is to minimize the statistical risk:
$$\sum\limits_{t=0}^T \mathbb{E}_{\mathcal{D}^{(t)}}[\gL(\phi_{t;\vtheta}(\sX^{(t)}), \sY^{(t)})],$$
where $\gL$ is a loss function applied to Neural Network $\phi_{t;\vtheta}$, with parameters $\vtheta$ optimized up to  time-step $t$,\footnote{While training is evaluated incrementally on the state of parameters $\vtheta$ at time-step $t$, inference is often assumed to be performed on the final model $\phi_{T-1;\vtheta}$.} and access to previous datasets $\mathcal{D}^{(t')}$, for $t' < t$, is limited or forbidden.
Restricting access to previous datasets makes the estimation of the statistical risk and its optimization challenging, as the empirical risk associated with a single dataset becomes an unreliable proxy of the entire distribution.
By imposing restrictions on the structure of the Incremental Learning problem, it is possible to define \textit{families of incremental settings}. Noting that the marginal distribution of inputs in the incremental framework satisfies $p(\sX^{(t)}) \not = p(\sX^{(t+1)})$ for each $0 \leq t < T-1$, three traditional settings can be defined by imposing a structure on the marginal distributions of outputs $p(\sY^{(t)})$.
\textbf{Class-incremental learning} (CIL) is characterized by $p(\sY^{(t)}) \not = p(\sY^{(t+1)})$ and $\sY^{(t)} \subset \sY^{(t+1)}$, meaning that each experience increases the number of labels available for classification, presenting new samples, possibly associated with different distributions.
\textbf{Domain-incremental learning} (DIL), imposes instead $p(\sY^{(t)}) = p(\sY^{(t+1)})$ and $\sY^{(t)} = \sY^{(t+1)}$, meaning that the output distribution is stationary over time (while the input distribution changes).
Finally, \textbf{Task-incremental learning} (TIL) requires $\sY^{(t)} \not = \sY^{(t+1)}$, meaning that each episode is characterized by disjoint mappings $\phi_{t;\vtheta}$ which have to be kept in memory (and also that a selector mechanism is required at inference time, to determine the proper mapping, based on the ``task-id'' $t$ for each sample).

\paragraph{Other Continual Learning Frameworks.}
Outside the boundaries of Incremental Learning, additional frameworks have been proposed for Continual Learning~\cite{wang2023comprehensive}, for instance, Task-free Continual Learning~\cite{aljundi2019task} does not rely on segregating each experience in discrete units, Blurred Boundary Continual Learning~\cite{bang2021rainbow} allows partial overlaps between experiences, Online Continual Learning~\cite{aljundi2019gradient} forces a single unshuffled pass over each dataset (dropping the i.i.d. observability assumption within each experience), and Continual Pre-training~\cite{sun2020ernie} focuses on continually improving generalization on future tasks. A wider perspective on the problem of learning over time is the one of Collectionless Artificial Intelligence~\cite{gori2023collectionless}, which focuses on lifelong learning with online learning dynamics in multi-agent distributed networks.

\section{Continual Learning Strategies}
Continual Learning tasks are characterized by additional technical requirements, compared to Machine Learning. Namely, implementations have to address the stability-plasticity trade-off by means of some non-trivial learning strategy, because of limited temporal observability and limited memorization capacity.
The stability-plasticity trade-off cannot be addressed in absolute terms and different Continual Learning tasks require different considerations. Moreover, although some strategies can be more beneficial than others on average, there is no single method dominating the others across every possible scenario, and, often, the best trade-off is achieved by combining multiple strategies together.
In this sense, Continual Learning strategies can be considered additional hyper-parameters which must be fine-tuned and validated, along with architecture and optimization values, for optimal performance at inference time.
Frameworks such as Avalanche~\cite{lomonaco2021avalanche} provide unified interfaces and evaluation settings which allow to compare the effect of different strategies on a given learning task.
%
Traditionally~\cite{de2021continual}, approaches in Continual Learning can be categorized into families of methods which are (i.) \textit{regularization-based}, (ii.) \textit{replay-based}, and (iii.) \textit{architecture-based}. More recently~\cite{wang2023comprehensive}, taxonomies have been extended with (iv.) \textit{optimization-based} and (v.) \textit{representation-based} strategies. 
%In the following, we will provide a high-level overview of each category, and cover the specific details of the methods which will be used in our experiments.

\paragraph{Regularization-based Methods.} Regularization-based strategies explicitly penalize forgetting in the optimization procedure, often requiring to keep in memory a frozen copy of the model at previous time-steps. In (i.) \textit{weight regularization} methods, some weight function decides, for each parameter of the Neural Network, how much it should be affected by a gradient update, based on the contribution of that parameter on the performance on old experiences. Conversely, (ii.) \textit{function regularization} methods counteract forgetting, by self-distillation of the model output (or intermediate representation) against an old copy, possibly with respect to training samples which were memorized during past experiences.
%\TODO{In this category we mention LwF}

\paragraph{Replay-based Methods.} Strategies based on replay work by means of representatives of the past, which are used to mitigate forgetting (i.e., they can be considered case-based reasoning approaches). (i.) \textit{Experience-replay} methods memorize a subset of training samples from past experiences, in a buffer populated according to some algorithm (which may or may not weight the decision by the contribution of each sample on the learning process), and then present a mini-batch of samples from past experiences which are optimized along with a mini-batch from the current experience. (ii.) \textit{Feature-replay} attempts to reduce memory requirements by storing previous samples in latent space, instead of input space, at the expenses of either freezing the feature extractor, or implementing additional strategies to counteract representational shift as learning progresses. In (iii.) \textit{generative-replay}, one or more generative models are trained to approximate the input distribution of each experience, allowing the learning agent to be trained on mini-batches of past experiences generated on the fly. 
%\TODO{In this category we mention Class-balanced Reservoir Sampling. Populates the buffer in an online fashion, in a way which asymptotically approximates an offline uniform sampling. Class-balanced additionally aaaaa}

\paragraph{Architecture-based Methods.} Architecture-based approaches explicitly partition the parameter space into different components, which can be dynamically disabled to prevent forgetting, when learning from new experiences. (i.) \textit{Parameter-allocation} methods explicitly segregate the parameter space in subsets which are enabled by binary masks different for each experience. (ii.) \textit{Model decomposition} approaches separate parameters into task-sharing components, which are always trained, and task-specific components, which are updated only for a given experience. (iii.) \textit{Modular networks} can dynamically instantiate additional components for each new experience encountered.

\paragraph{Optimization-based Methods.} Optimization-based methods alter the behavior of the optimizer, to preserve the direction of local minima of previous experiences.
In (i.) \textit{gradient projection} strategies, the gradient of the current experience is altered to be orthogonal with respect to gradients computed on past experiences (often in combination with Replay-based methods to avoid storing past gradients). (ii.) \textit{Meta-learning} approaches attempt to control inductive biases and hyper-parameters in a data-driven fashion. (iii.) \textit{Loss landscape} methods exploit topological features of past and current loss landscapes (e.g., local minima) to guide the optimizer along a path of low error.
%\TODO{In this category we mention GEM, which traditionally was classified as Replay-based}

\paragraph{Representation-based Methods.} Representation-based strategies attempt to learn initial strong representations, which can mitigate forgetting by avoiding large representational shifts during successive experiences. Methods for (i.) \textit{self-supervised learning} in a Continual Learning setting can provide more robust representations thanks to contrastive training regimes. (ii.) Offline \textit{pre-training} can be beneficial to a downstream Continual Learning experience, in virtue of stronger initial representations, smaller gradients and the possibility of freezing a portion of the architecture. Dually, (iii.) \textit{Continual pre-training} collects data in an incremental fashion to improve the pre-training phase of an architecture performing some atemporal downstream task.


\section{Continual Learning Metrics}
Evaluation in Continual Learning requires to characterize (i.) \textit{performance}, (ii.) \textit{stability} and (iii.) \textit{plasticity}, as the agent is exposed to new experiences over time.
Starting from an atomic performance metric evaluated for experience $j$, such as the micro-accuracy:\footnote{TP: true positives, TN: true negatives, FP: false positives, FN: false negatives.}
\begin{equation*}
	\textsc{acc}_j = \frac{1}{2}\cdot \frac{TP_j}{TP_j + FN_j} + \frac{1}{2}\cdot \frac{TN_j}{TN_j + FP_j},
\end{equation*}
it is possible to build an \textsc{accuracy matrix} $\mA: \emA_{i,j} = \textsc{ACC}^{(i)}_j$, collecting the performance evaluated at each time-step $i$, for every experience $j$.\footnote{This means that the diagonal ($i = j$) summarizes performances on the current experience, the lower triangular matrix ($i > j$) provides information related to stability of past experiences, and the upper triangular matrix ($i < j$) can act as proxy of plasticity for yet-to-be-observed experiences.}
In the Continual Learning literature~\cite{wang2023comprehensive,mai2022online}, the \textsc{accuracy matrix} is condensed in sequences of quantities which can be more readily interpreted as proxies for performance at the end of each training experience, stability and plasticity, over time. Evaluating metrics at the last time-step (i.e., at the end of the entire training process), further condenses such quantities into scalars.
In the following, we briefly summarize only the Continual Learning metrics which will be used in the experimental part of this thesis.
\paragraph{\textsc{Average Accuracy}.} Average accuracy summarizes the performance over time, across every experience:
\begin{equation*}
	\textsc{Average\_Accuracy}^{(t)} = \frac{1}{t} \sum_{j=0}^t \emA_{t,j}.
\end{equation*}

\paragraph{\textsc{Average Forgetting}.} Average forgetting compares the current performance with the best performance observed in the past. \textbf{Lower} values are preferable. Let $\textsc{ACC}^*_j = \displaystyle\max_{1 \leq k < j} \emA_{k,j}$, then \textsc{Average Forgetting} is defined as:
\begin{align*}
	\textsc{Average\_Forgetting}^{(0)} &= 0\\
	\textsc{Average\_Forgetting}^{(t)} &= \frac{1}{t - 1} \sum_{j=1}^{t-1} (\textsc{ACC}^*_t - \emA_{t,j}).
\end{align*}

\paragraph{\textsc{Positive Backward Transfer}.} Backward transfer summarizes the agent stability, by measuring the positive influence of the current learning experience on the performance on past experiences. \textbf{Higher} values are preferable. \textsc{Positive Backward Transfer} is defined as:
\begin{align*}
	\textsc{Backward\_Transfer}^{(0)} &= 0\\
	\textsc{Backward\_Transfer}^{(t)} &= \left(\frac{2}{t \cdot (t - 1)}\cdot \sum_{j=1}^{t} \sum_{i=0}^{j-1} (\emA_{i,j} - \emA_{j,j}) \right)^+.
\end{align*}

\paragraph{\textsc{Forward Transfer}.} Forward transfer assesses plasticity, by measuring the positive influence of the current learning experience on the performance on future experiences. \textbf{Higher} values are preferable, the value can be negative, in case of destructive interference. Let $\textsc{ACC}_j^\Delta$ be the performance of a random classifier on task j,\footnote{We will assume $\textsc{ACC}_j^\Delta = 0.5$ in the experiments described in this thesis.} then \textsc{Forward Transfer} can be defined as:
\begin{equation*}
	\textsc{Forward\_Transfer}^{(t)} = \frac{2}{t \cdot (t - 1)}\cdot \sum_{j=0}^{t-1} \sum_{i=j+1}^{t} (\emA_{i,j} - \textsc{ACC}_j^\Delta).
\end{equation*}

\section{Continual Interpretable-by-Design Methods} There is limited literature available at the intersection between interpretable-by-design methods~\cite{rudin2022interpretable} and Continual Learning.
In the context of Concept Bottleneck Models, Marconato et al.~\cite{marconato2022catastrophic} investigated the effect of concept supervision on catastrophic forgetting in a Class-incremental setting. %They define the metrics of concept average accuracy and concept divergence, the latter being the Kullback-Leibler divergence between concept distributions at time $t$ and $t_0$. %and point out the beneficial effect of concept supervision in mitigating forgetting.
Marconato et al. have also proposed the first application of Continual Learning to Neuro-symbolic Artificial Intelligence~\cite{marconato2023neuro}. Their framework is able to learn and stabilize concepts in a Class-incremental setting where disjoint sets of concepts are presented at each task. They empirically show how knowledge-driven approaches are more susceptible to catastrophic forgetting, but also tend to benefit more than purely-neural methods from Continual Learning strategies. %However they also show that, without a mechanism to prevent concept drift, even knowledge-based solutions can fail.
Their proposed solution relies on concept annotations and exact background knowledge, and it is based on the combined use of a Replay Buffer for task labels and Pseudo-rehearsal in the form of a Kullback-Leibler divergence term between concept distributions predicted by the current model and a snapshot on previous tasks. %Crucially, their method requires strong annotations in the form of a densely annotated (at the concept level) core dataset or exact background knowledge (which alone is still insufficient if tasks are affected by reasoning shortcuts).
%Compared to the works of~\cite{marconato2022catastrophic,marconato2023neuro}, which strongly rely on concept supervision (albeit sporadic, if combined with background knowledge) and evaluate their approaches on relatively large datasets spread across a limited temporal dimension (3 to 8 tasks), our work targets an unsupervised setting, where domain knowledge is unavailable and information is extracted gradually, over a significantly longer temporal horizon (20+ tasks) and with fewer samples.
%Thanks to the unsupervised setting, we make no assumption on the correctness of ground truth concepts, while their approach heavily relies on both concepts and background knowledge to be exact.
%
In the domain of Reinforcement Learning, Zabounidis et al.~\cite{zabounidis2023concept} developed a concept-based policy for multi-agent settings, enabling a high-degree of interpretability and test-time interventions. Their approach assumes an oracle capable of annotating every observation with the full set of concept.%s, while our approach aims to learn concepts independently, from observations which disclose them gradually. 
%Prototype-based Models~\cite{chen2019looks} are another family of interpretable models, alternative to CBMs, which store input portions for the sake of similarity-based inference. Unlike CBMs, the input patches are discovered in an unsupervised fashion.
Rymarczyk et al.~\cite{rymarczyk2023icicle} extend a Prototype-based approach to the Class-incremental setting, achieving a higher performance compared to non-continual baselines, without storing exemplars of previous classes.
Their approach is based on separate Prototype Layers for each task and three regularization terms in the loss function. The first term penalizes interpretability drift, by comparing the activation scores of a stored snapshot and the current network, selecting only the highest pixel activations by means of a binary mask. The second term initializes new prototypes to be similar to already learned ones. Finally, the last term rescales logits of previous tasks to account for backbone change over the last task. Akin to traditional Prototype-based methods, training cannot proceed in an end-to-end fashion, and it includes a separate convex optimization of the last layer.
%\textcolor{red}{We initially investigated a binarized masking mechanism for concepts, however it proved to be ineffective when combined with Hamming triplet loss.}
%Our setting is orthogonal% to the one by~\cite{rymarczyk2023icicle}
%, as we allow memorization of examples, but do not allocate separate concept layers for each new task,\footnote{Our task heads separate after the concept bottleneck, thus concepts are shared across all the tasks.} opening up to possibly lifelong settings.

\paragraph{Unsupervised Concept Discovery over Time.} Locatello et al.~\cite{locatello2019challenging} provided important theoretical contributions on the unfeasibility of generalized Unsupervised Disentanglement Learning, without inductive biases both on architecture and datasets. In a large-scale experiment on Variational Auto-encoders across multiple datasets, they show that no method dominates the other and that successful disentanglement strongly depends on hyper-parameter tuning and random initialization. Although successful disentanglement can only be assessed with supervised metrics, they remark that model selection cannot exploit supervisions at the latent level and that unsupervised metrics (like reconstruction error or mutual information) are ineffective.
They also note how %, for variational autoencoder-based approaches,
disentanglement (as measured by supervised metrics) correlates with sample complexity on downstream tasks, but this correlation may be a spurious result of metrics capturing also some form of ``informativeness'' of the latent space.
%
More recently, Horan et al.~\cite{horan2021unsupervised} identified two sufficient conditions (well known in the field of Manifold Learning) which make Unsupervised Disentanglement Learning consistently achievable: local isometry and non-Gaussianity of latent factors. In their experiments they show how these conditions are satisfied by many natural image manifolds.% and that Hessian EigenMaps (HLLE algorithm) followed by fastICA consistently retrieve disentangled representations, with vanilla autoencoders failing to do so, in spite of low reconstruction errors and high-quality outputs.
%
They also propose a teacher-student approach, where a traditional Auto-encoder (the student) is trained to encode the input image and decode the output from a latent space computed by the teacher. %computed by HLLE+ICA
%
Although with a much higher number of samples, their approach consistently produces disentangled representations, while the same architecture, without their teacher-student paradigm, does not.
Li et al.~\cite{li2022continual} developed a Bayesian Self-organizing Map exploiting snapshots and a fuzzy masking mechanism to achieve good disentanglement in a Continual Learning setting. %They evaluate their method on multiple datasets, however the continual component is artificially introduced by splitting datasets in two time steps \textcolor{red}{SE NON HO CAPITO MALE}.
%
%As we focus on deterministic and logically-interpretable representations, our method strongly differs from disentanglement of generative models. However, similar methodological constraints must be enforced, and the same difficulties are present in evaluating the goodness of representations without supervised information. %Additionally, our setting is further constrained by the assumption of data efficiency, as we strive to achieve unsupervised concept learning with few samples, distributed continually over time.
%\textcolor{red}{We initially investigated a masking mechanism similar to~\cite{li2022continual} to promote concept alignment, however it proved to be ineffective when combined with the Hamming triplet loss. Vogliamo dirlo?} \textcolor{blue}{Mah, secondo me no...}

%\paragraph{Experience replay} aaa

%In contrast with existing techniques, our concept replay buffer ???

%\paragraph{Deep hashing} aaa
